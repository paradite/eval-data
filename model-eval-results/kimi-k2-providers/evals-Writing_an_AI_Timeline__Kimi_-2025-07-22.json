{
  "evals": [
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "claude-sonnet-4-20250514",
      "provider": "anthropic",
      "response": "2015 - Foundation Year for Modern AI\n\nDQN (Deep Q-Network) - Google DeepMind published the seminal paper introducing Deep Q-Networks, which successfully combined deep learning with reinforcement learning to achieve human-level performance on Atari games, marking a breakthrough in deep reinforcement learning.\n\nResNet - Microsoft Research introduced Residual Networks (ResNet), solving the vanishing gradient problem in very deep neural networks through skip connections, enabling training of networks with hundreds of layers and achieving breakthrough performance on ImageNet.\n\n2016 - The Year AI Beat Human Champions\n\nAlphaGo - Google DeepMind created AlphaGo, the first computer program to defeat a professional human Go player (Lee Sedol), combining Monte Carlo tree search with deep neural networks trained by supervised and reinforcement learning.\n\nOpenAI Formation - OpenAI was founded as a non-profit AI research company by Elon Musk, Sam Altman, and others, with the stated mission to ensure artificial general intelligence benefits all of humanity.\n\n2017 - The Transformer Revolution Begins\n\nTransformer Architecture - Google researchers published \"Attention is All You Need,\" introducing the Transformer architecture that would become the foundation for all major language models, revolutionizing natural language processing through self-attention mechanisms.\n\nAlphaGo Zero - DeepMind developed AlphaGo Zero, which learned to play Go from scratch through self-play without any human game data, achieving superhuman performance and demonstrating the power of pure reinforcement learning.\n\nPPO (Proximal Policy Optimization) - OpenAI introduced Proximal Policy Optimization, a policy gradient method that became one of the most widely used reinforcement learning algorithms due to its simplicity and effectiveness.\n\n2018 - Language Models Emerge\n\nBERT - Google AI introduced BERT (Bidirectional Encoder Representations from Transformers), demonstrating that pre-training transformer models on large text corpora and fine-tuning on specific tasks could achieve state-of-the-art results across multiple NLP benchmarks.\n\nGPT-1 - OpenAI released the first Generative Pre-trained Transformer, showing that unsupervised pre-training of transformer models on diverse text could produce coherent text generation and transfer learning capabilities.\n\nOpenAI Five - OpenAI developed OpenAI Five, an AI system that defeated professional Dota 2 teams, showcasing reinforcement learning's capability in complex, real-time strategy games with partial information.\n\n2019 - Scaling Up Language Models\n\nGPT-2 - OpenAI released GPT-2, a significantly larger language model with 1.5 billion parameters that generated remarkably coherent text, initially withheld from public release due to concerns about potential misuse.\n\nAlphaStar - DeepMind created AlphaStar, which achieved Grandmaster level in StarCraft II, demonstrating advanced multi-agent reinforcement learning and long-term strategic planning in complex environments.\n\nT5 - Google introduced T5 (Text-to-Text Transfer Transformer), framing all NLP tasks as text-to-text problems and showing the versatility of the transformer architecture across diverse language tasks.\n\n2020 - The Scale Revolution\n\nGPT-3 - OpenAI released GPT-3 with 175 billion parameters, demonstrating emergent abilities like few-shot learning and showing that language models could perform various tasks without task-specific training, sparking widespread interest in large language models.\n\nDALL-E - OpenAI introduced DALL-E, a transformer-based model capable of generating images from text descriptions, extending the success of transformers beyond text to multimodal applications.\n\nAlphaFold 2 - DeepMind's AlphaFold 2 achieved breakthrough accuracy in protein structure prediction, solving a 50-year-old grand challenge in biology and demonstrating AI's potential for scientific discovery.\n\n2021 - Multimodal AI and Code Generation\n\nCLIP - OpenAI released CLIP (Contrastive Language-Image Pre-training), which learned visual concepts from natural language supervision, enabling zero-shot image classification and bridging vision and language understanding.\n\nGitHub Copilot - GitHub and OpenAI launched Copilot, an AI pair programmer based on OpenAI Codex that could generate code from natural language descriptions, revolutionizing software development workflows.\n\nPaLM - Google introduced PaLM (Pathways Language Model) with 540 billion parameters, demonstrating improved reasoning capabilities and multilingual understanding through increased scale.\n\n2022 - AI Goes Mainstream\n\nChatGPT Launch - OpenAI released ChatGPT based on GPT-3.5, making conversational AI accessible to the general public and sparking unprecedented mainstream adoption of AI technology with over 100 million users in two months.\n\nRLHF Breakthrough - OpenAI's InstructGPT paper demonstrated how Reinforcement Learning from Human Feedback (RLHF) could align language models with human preferences, making them more helpful, harmless, and honest.\n\nDALL-E 2 - OpenAI released DALL-E 2, significantly improving image generation quality and resolution while introducing techniques like inpainting and style transfer, popularizing AI art generation.\n\nStable Diffusion - Stability AI released Stable Diffusion as an open-source text-to-image model, democratizing access to high-quality image generation and spurring widespread experimentation and adoption.\n\n2023 - The Year of GPT-4 and Competition\n\nGPT-4 - OpenAI released GPT-4, demonstrating multimodal capabilities (text and images), improved reasoning, and passing professional exams like the bar exam, representing a significant leap in language model capabilities.\n\nClaude - Anthropic released Claude, an AI assistant trained using Constitutional AI techniques to be helpful, harmless, and honest, introducing alternative approaches to AI safety and alignment.\n\nBard Launch - Google launched Bard, their conversational AI service based on LaMDA and later PaLM, entering the consumer chatbot market in response to ChatGPT's success.\n\nGPT-4 Turbo - OpenAI released GPT-4 Turbo with a 128K context window and updated knowledge cutoff, significantly expanding the model's ability to process and reason over long documents.\n\n2024 - Multimodal AI and Advanced Reasoning\n\nGPT-4o - OpenAI introduced GPT-4o (\"omni\"), a natively multimodal model capable of processing text, audio, and vision in real-time, representing a major step toward more natural human-AI interaction.\n\nClaude 3 - Anthropic released Claude 3 (Opus, Sonnet, Haiku), with Claude 3 Opus matching or exceeding GPT-4 performance on many benchmarks while maintaining strong safety properties through constitutional AI training.\n\nGemini Ultra - Google released Gemini Ultra, claiming state-of-the-art performance across multiple modalities and becoming the first model to exceed human expert performance on the MMLU benchmark.\n\no1 Models - OpenAI introduced the o1 series (o1-preview and o1-mini), featuring enhanced reasoning capabilities through reinforcement learning, showing significant improvements on complex mathematical and scientific problems.",
      "reasoning": "",
      "rating": 9.5,
      "notes": "Covers almost all points",
      "usage": {
        "promptTokens": 223,
        "completionTokens": 1621,
        "totalTokens": 1844,
        "thoughtsTokens": 0
      },
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "images": [],
      "durationMs": 31037,
      "id": "c5b374da-38cb-4b21-9a95-773a71af8990",
      "createdAt": "2025-07-18T07:09:28.298Z",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "novita"
        ]
      },
      "response": "2015 - DQN Publised\nDQN - DeepMind’s Volodymyr Mnih et al. published “Human-level control through deep reinforcement learning,” showing neural networks could learn to play Atari games end-to-end, sparking the modern RL renaissance.  \nImageNet ResNet - Kaiming He’s team at Microsoft Research Asia introduced residual connections with ResNet, enabling the first > 3 % improvement on ImageNet and making very deep transformers possible.  \nAlphaGo Lee - Still in its training iteration, DeepMind’s AlphaGo became the first program to defeat European champion Fan Hui in October 2015.\n\n2016 - AlphaGo vs Lee Sedol\nAlphaGo Sedol - DeepMind beat world-champion Lee Sedol 4–1 in Seoul, captivating global attention and demonstrating that neural+RL hybrids can exceed expert humans in complex strategy games.  \nOpenAI Gym - Greg Brockman, Ilya Sutskever and others launched OpenAI Gym, the standard open-source environment suite that let researchers benchmark RL algorithms on identical tasks.  \nImportance Weighted Actor-Learner (IMPALA) - DeepMind began preparatory work on the scalable distributed actor-learner architecture later released as IMPALA, moving high-throughput RL from single-GPU labs to industrial scale.\n\n2017 - Attention is All You Need\nTransformer - Google Brain’s Vaswani et al. published “Attention Is All You Need,” inventing the self-attention mechanism that powers every subsequent LLM.  \nAlphaZero - DeepMind replaced handcrafted Go knowledge in AlphaGo Lee with pure self-play reinforcement learning, producing AlphaZero, the most general game learner to date.  \nProximal Policy Optimization (PPO) - John Schulman at OpenAI released PPO, becoming the workhorse policy-gradient method for RL-based pretraining and fine-tuning used across industry.\n\n2018 - BERT & Self-Supervised Pre-training\nBERT - Google AI Language under Jacob Devlin developed BERT: bidirectional pre-training of transformers on masked language tasks, proving the value of large-scale self-supervised learning for language.  \nOpenAI Five - OpenAI showed RL agents could reach pro-level Dota 2 using PPO on 256 GPUs across 180 years of self-play per day, highlighting the compute explosion of the era.  \nTrust-Region Methods and Soft Actor-Critic (SAC) - Tuomas Haarnoja and others published SAC, delivering off-policy RL with sample-efficient, entropy-regularized policies—a staple for robotics and fine-tuning.\n\n2019 - GPT-2 & The Scaling Hypothesis\nGPT-2 - OpenAI’s Alec Radford and Jeff Wu introduced GPT-2 (1.5 B parameters), demonstrating that bigger transformers pretrained on web text can generate coherent long passages; the model was initially withheld “for safety.”  \nGPT - Simultaneous work laid out the generative pretraining + supervised fine-tuning recipe—the precursor to RLHF—used in GPT and GPT-2.  \nMuZero - DeepMind unveiled MuZero, learning accurate world models without access to any rules of Go, Atari or chess, an early glimpse at internal simulations for model-based RL.\n\n2020 - Stagger of Scale\nAlphaFold 2 - DeepMind’s breakthrough protein-structure predictor showed deep learning could radically accelerate science.  \nGPT-3 - OpenAI released the 175 B parameter GPT-3: first system to exhibit “few-shot” generalization without gradient updates, consolidating the scaling hypothesis.  \nCLIP & DALL-E - Alec Radford, Aditya Ramesh and others released CLIP and DALL-E, showing unified vision-language foundations would expand generative AI.\n\n2021 - RLHF Arrives\nInstructGPT - OpenAI used PPO + human feedback (RLHF) to fine-tune GPT-3 into InstructGPT, achieving 100× human preference gains and establishing RLHF as the dominant post-training method for chat models.  \nT5 & PaLM - Google refined encoder-decoder transformers with T5 (11 B) and later PaLM (540 B), pushing sparse-attention and mixture-of-experts scaling.  \nAlphaStar Unplugged - DeepMind released general reinforcement learning from human privilege data, nudging commercial RL toward offline pre-training + on-policy adaptation.\n\n2022 - Chatbots Go Mainstream\nChatGPT - OpenAI shipped ChatGPT, a GPT-3.5 model fine-tuned with RLHF, attracting 100 M users in two months and crystallizing LLMs as consumer products.  \nLaMDA & Sparrow - Google and DeepMind released dialogue-oriented LLMs using RL policies that reward safety and factual grounding.  \nConstitutional AI - Anthropic introduced Constitutional AI, a form of RLHF in which models critique their own outputs from a written “constitution,” later used to train Claude.\n\n2023 - Claude & the Multimodal Wave\nClaude - Anthropic released Claude 1 and Claude 2 large language assistants trained with Constitutional AI, emphasizing harmlessness and long-context support (100 k tokens).  \nGPT-4 - OpenAI launched GPT-4 (March), a multimodal transformer with RLHF and rule-based reward modeling; it scored 5 on AP exams and near-top on Bar exams.  \nLlama 2 & Llama Chat - Meta open-sourced Llama-2-70B, including a variant fine-tuned with RLHF, giving the community its first major open weights RLHF chat model.\n\n2024 - o1 & Scaling Reasoning\no1 - OpenAI unveiled the “o1” reasoning series, using chain-of-thought reinforcement learning and expansive inference-time compute to exceed PhD-level accuracy on STEM olympiads.  \nClaude 3.5 Sonnet - Anthropic updated Claude with near-human code-writing skills and frontier performance on SWE-bench.  \nGemini 1.5 Pro - Google released a 1 M token context window model with MoE architecture, demonstrating ultra-long-context RL-labeled instruction tuning paired with safe RLHF.",
      "reasoning": "",
      "usage": {
        "promptTokens": 190,
        "completionTokens": 1243,
        "completionTokensWithoutThoughts": 1243,
        "totalTokens": 1433,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 109908,
      "id": "b07a07da-876a-44d9-9c02-0e4e9b938bcf",
      "createdAt": "2025-07-18T14:00:41.582Z",
      "notes": "Covers most points\nWrong format",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 9,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "parasail"
        ]
      },
      "response": "2015 - Deep Q-Networks beat Atari\n\nDQN Nature paper - Google DeepMind published “Human-level control through deep reinforcement learning,” showing that a single neural net trained with Q-learning could exceed human performance on 49 Atari games without hand-crafted features, anchoring deep RL as a scalable paradigm.\n\nOpenAI founded - Sam Altman, Greg Brockman, Ilya Sutskever et al. launched OpenAI as a non-profit with the mission of ensuring artificial general intelligence benefits humanity and began publishing influential RL work such as GAIL and Evolution Strategies.\n\n2016 - AlphaGo topples Go world champion\n\nAlphaGo - Google DeepMind created AlphaGo, the first program to defeat a professional human Go player (Fan Hui) and then world champion Lee Sedol, combining Monte Carlo tree search with deep policy and value networks trained by supervised and reinforcement learning.\n\nOpenAI Gym - OpenAI released Gym, an open-source toolkit for comparing RL algorithms with standardized benchmarks including Atari, MuJoCo and classic control.\n\n2017 - Transformers introduced and AlphaGo Zero learns from self-play\n\nTransformer paper - Google Brain researchers including Ashish Vaswani, Jakob Uszkoreit and Illia Sutskever published “Attention Is All You Need,” replacing recurrent layers with self-attention, yielding the Transformer architecture that would underpin modern language models.\n\nAlphaGo Zero - DeepMind retired human-data training and produced AlphaGo Zero, which reached superhuman play tabula rasa through pure self-play and a dual-headed policy-value network using MCTS and ResNets.\n\nProximal Policy Optimization - OpenAI proposed PPO, a clipped surrogate-objective policy-gradient algorithm that became the default workhorse for on-policy RL across domains.\n\n2018 - BERT ignites bidirectional pre-training\n\nBERT - Google Research released the Bidirectional Encoder Representations from Transformers model, showing that masked-language-model pre-training + fine-tuning transferred widely to NLP benchmarks, popularizing large-scale language pre-training.\n\nImpala - DeepMind introduced IMPALA, a massively distributed actor-critic architecture demonstrating high-throughput deep RL on multi-task environments like DMLab.\n\nGPT-1 - OpenAI published “Improving Language Understanding by Generative Pre-Training,” the first GPT model (117 M parameters) trained on BooksCorpus, demonstrating unsupervised pre-training then supervised fine-tuning.\n\n2019 - Large-scale generative pre-training advances and OpenAI Five Dota 2\n\nGPT-2 - OpenAI trained a 1.5 B parameter generative Transformer able to produce coherent long-form text; staged release due to “concerns of malicious use” highlighted the power of scaling data and compute.\n\nOpenAI Five Dota 2 - OpenAI defeated the world-champion Dota 2 team OG using 45,000 years of self-play RL with systems co-trained via PPO-variants and reward shaping, showing deep RL on high-dimensional strategy games.\n\nMuZero - DeepMind unveiled MuZero, which learns latent dynamics and achieves superhuman performance in Atari, Go, Chess and Shogi without ever seeing the underlying rules, marrying model-based RL with search.\n\n2020 - Scaling Laws formalized and GPT-3 captivates the world\n\nGPT-3 - OpenAI released the 175 B parameter GPT-3 in the paper “Language Models are Few-Shot Learners,” displaying in-context learning and prompting capabilities across dozens of tasks without fine-tuning, popularizing prompt engineering.\n\nJurassic-1 - AI21 Labs launched Jurassic-1 (178 B), the first large-scale code-switching model to challenge GPT-3 and highlight competition beyond OpenAI.\n\nScaling Laws paper - OpenAI researchers including Jared Kaplan and Dario Amodei published empirical power-law relationships between model size, data, compute and performance, guiding the surge to trillion-parameter-class models.\n\nCLIP & DALL-E - OpenAI released CLIP for joint vision-language understanding and DALL-E for text-to-image generation, foreshadowing multimodal LLMs.\n\n2021 - Codex, InstructGPT and Constitutional AI reroute alignment\n\nCodex - OpenAI released the 12 B parameter Codex, fine-tuned on GitHub code, powering GitHub Copilot and establishing LLMs for programming assistance.\n\nInstructGPT - OpenAI used human-written demonstrations and ranking (RLHF) to fine-tune GPT-3 into InstructGPT, showing that small-scale RL-tuning dramatically improves helpfulness and safety while reducing toxicity.\n\nConstitutional AI - Anthropic introduced Constitutional AI, using self-critique and preference learning via RLHF with a set of constitutional principles to align Claude models without manually labeled harm examples.\n\nPaLM & Gopher - Google trained PaLM (540 B) and DeepMind scaled Gopher (280 B), setting new NLP SOTA and demonstrating data-efficiency at unprecedented scale.\n\n2022 - ChatGPT and RLHF made conversational AI accessible\n\nChatGPT - OpenAI released ChatGPT, a fine-tuned GPT-3.5 model that combined instruction tuning and RLHF, reaching 100 M users in two months and turning LLMs into a mainstream product.\n\nClaude initial launch - Anthropic launched Claude, a Constitutional-AI aligned assistant emphasizing safety via AI-assisted oversight and red-teaming.\n\nStable Diffusion & Midjourney – Emad Mostaque-led Stability AI, together with LAION, open-sourced Stable Diffusion, democratizing text-to-image generation; Midjourney gained mass adoption, illustrating diffusion models complementing LLMs.\n\nSparrow - DeepMind revealed Sparrow, an 80 B parameter dialogue agent fully trained via RLHF plus rule-based rewards that aimed to cite accurate evidence and reduce harm, previewing safer LLMs.\n\n2023 - GPT-4 marks “sparks of AGI” and Claude 2 scales context\n\nGPT-4 - OpenAI released GPT-4 (1.7 T rumors) with multimodal reasoning on text, images and code, scaling context to 32 K tokens, passing human-level exams and prompting claims of “sparks of artificial general intelligence.”\n\nClaude 2 - Anthropic launched Claude 2 with 100 K token context windows and upgraded Constitutional AI data, competing openly against GPT-4 and sharpening the safety-capability discourse.\n\nGoogle Bard & Gemini - Google responded with Bard, then merged DeepMind & Brain into Google DeepMind and unveiled Gemini 1.0 Ultra, claiming to surpass GPT-4 on many benchmarks via training with Google’s TPU v5e fleet.\n\nLlama 2 & Code Llama - Meta open-sourced Llama 2 (7-70 B) and Code Llama, releasing commercial permissive weights that fueled thousands of derivatives and kicked off the modern open LLM ecosystem.\n\nConstitutional missing doc review - Anthropic released the Constitutional AI dataset and evaluation, providing public tools for researchers to build safer instruction models.\n\n2024 - Multimodal frontiers, long-context wars, and frontier regulation\n\nClaude 3 - Anthropic launched Claude 3 Haiku, Sonnet and Opus models with an initial 200 K context window extensible to 1 M tokens, achieving new SOTA on MMLU, coding and vision tasks while emphasizing robust alignment evaluations.\n\nGPT-4o & o1 series - OpenAI released GPT-4o (omni-modality) with native audio and vision conversation at voice latencies, and the o1 series that prunes broad pre-training for long-horizon problem solving via test-time RL via Monte Carlo tree search.\n\nGemini 1.5 Pro - Google DeepMind introduced Gemini 1.5 Pro with 1 M-token sparse Mixture-of-Experts context handling and breakthrough long-context retrieval, solidifying TPU-centric scaling.\n\nDALL-E 3, Sora diffusion and regulation - OpenAI integrated DALL-E 3 into ChatGPT, previewed Sora text-to-video models, drove policy debates on AI safety and EU AI Act adoption, while Frontier Model Forum (Microsoft, OpenAI, Anthropic, Google) began early voluntary standards.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1646,
        "completionTokensWithoutThoughts": 1646,
        "totalTokens": 1845,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 188085,
      "id": "104503f8-0c4f-4dac-99b0-25053c36211f",
      "createdAt": "2025-07-18T14:19:09.636Z",
      "notes": "Covers most points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 9.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "groq"
        ]
      },
      "response": "2015 - Deep Learning Ascendant\n\nDQN - DeepMind published \"Human-level control through deep reinforcement learning\", showing that a single neural agent playing dozens Atari games at super-human strength could be trained end-to-end from nothing but pixels and a reward signal, introducing the world to deep Q-networks.\n\nGo progress - Facebook published research on a CNN Go bot that beat strong amateurs, foreshadowing bigger things to come.\n\nOpenAI founded - Sam Altman, Greg Brockman, Ilya Sutskever and others launched OpenAI as a non-profit dedicated to developing safe artificial general intelligence.\n\n2016 - Super-human Skill in Go\n\nAlphaGo - Google DeepMind created AlphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\nAlphaGo Zero - An improved self-play version that removed the need for human data arrived only months later, showing pure RL could reach superhuman levels.\n\n2017 - Attention Is All You Need\n\nTransformer - Google Brain's Ashish Vaswani et al. published “Attention Is All You Need”, replacing recurrence and convolution with the scalable self-attention mechanism that would soon power large language models. A modest 117 million-parameter “Transformer” set new records on WMT translation benchmarks.\n\nPPO - OpenAI released Proximal Policy Optimization (PPO), a simple yet robust policy-gradient RL algorithm that quickly became a standard workhorse for robotics and games.\n\n2018 - Bidirectional Language Understanding\n\nBERT - Google introduced BERT (Bidirectional Encoder Representations from Transformers) – pre-trained with a fill-in-the-blank objective – and showed massive gains across reading-comprehension, QA and classification tasks, demonstrating the power of unsupervised pre-training and fine-tuning on the new Transformer backbone.\n\nGPT-1 - OpenAI published “Improving Language Understanding by Generative Pre-Training”, releasing a Transformer decoder with 117 million parameters trained on a large corpus to model next-token probabilities, setting the first GPT milestone.\n\n2019 - Scaling Up & Open-AlphaStar\n\nGPT-2 - OpenAI’s 1.5-billion parameter GPT-2 was initially withheld for “safety” concerns, but once released showcased eerily coherent long-form text generation and brought the phrase “large language model” into popular use.\n\nAlphaStar - DeepMind unveiled AlphaStar, an RL-trained agent that achieved Grandmaster level in the real-time strategy game StarCraft II under full match conditions, using population-based training and human-replay supervision.\n\nT5 - Google’s “Text-to-Text Transfer Transformer” later in 2019 unified every NLP task under the same encoder-decoder paradigm using the Transformer.\n\n2020 - The 175B Parameter Breakthrough\n\nGPT-3 - OpenAI released GPT-3 with 175 billion parameters, demonstrating that in-context few-shot learning emerges at scale and giving the first commercial glimpse of LLMs as general-purpose assistants. The API beta instantly spurred thousands of applications.\n\nAlphaFold 2 - DeepMind, applying attention-like evidential networks, solved the 50-year-old problem of predicting protein 3D structure from sequence, showcasing the broad utility of scale and attention.\n\nCLIP & DALL-E - OpenAI introduced the contrastive vision-language model CLIP and the first DALL-E model, showing how large multimodal pre-training could enable zero-shot image generation and recognition.\n\n2021 - Multimodality & Reinforcement Learning from Human Feedback\n\nInstructGPT - OpenAI applied RLHF to GPT-3, fine-tuning it to follow instructions and align with human preferences while reducing toxic outputs.\n\nAlphaCode - DeepMind released an RL-trained transformer system that competes with human programmers on competitive programming challenges.\n\nCodex - OpenAI launched Codex, a GPT descendant fine-tuned on GitHub code, powering the first version of GitHub Copilot.\n\nFLAN-T5 - Google introduced instruction-finetuned T5 showing that task instructions alone can generalize zero-shot to hold-out tasks.\n\nWebGPT - OpenAI trained GPT to search the web and answer questions with citations, injecting retrieval-augmented generation into the LLM era.\n\n2022 - Alignment, Diffusion and Chat\n\nChatGPT - OpenAI combined RLHF, safety systems and a dialogue-optimized variant of GPT-3.5 into ChatGPT, hitting 100 million users in two months.\n\nPalm - Google unveiled PaLM (540 billion parameters) and PaLM 2 later in the year, showcasing multilingual and reasoning capabilities.\n\nAlphaTensor - DeepMind discovered faster matrix-multiplication algorithms for common sizes via a Transformer-based RL agent, blurring math and machine learning.\n\nStable Diffusion - Stability AI released the latent-diffusion text-to-image model, democratizing creative AI with open-source weights.\n\nChinchilla - DeepMind’s paper “Training Compute-Optimal Large Language Models” re-focused the field on data-scaling efficiency rather than parameters alone.\n\n2023 - The GPT-4 & Claude Wave\n\nGPT-4 - OpenAI released a large multimodal model (text & vision) that sets the bar for exams, coding interviews, and safety benchmarks, followed by the GPT-4-Turbo upgrade and the plugin ecosystem.\n\nClaude - Anthropic publicly launched Claude, emphasizing Constitutional AI and safety methods such as self-critique and RL from AI feedback.\n\nLlama / Llama 2 - Meta open-sourced the Llama series, enabling academics and startups to build on top of powerful base LLMs.\n\nGemini - Google DeepMind merged its Brain & DeepMind divisions and launched Gemini Ultra, claiming state-of-the-art across text, code, image and video benchmarks.\n\nHuman Feedback & Alignment Research - Anthropic, OpenAI, and DeepMind published further advances in RLHF, RL from AI Feedback (RLAIF) and Constitutional AI, forming the backbone for safer large-scale deployment.\n\n2024 - Reasoning, Long-Context & Open Weights\n\nClaude 3 - Anthropic’s Claude 3 family (Haiku, Sonnet, Opus) re-set standards in long-context (200k+ tokens), safety and coding benchmarks.\n\nGPT-4o - OpenAI released “o” for omni (audio+vision realtime) and a distilled “o1” reasoning model that “thinks” before responding, tackling science and code tasks at human expert levels.\n\nGemini 1.5 - Google DeepMind achieved a >1M-token context window and Mixture of Experts scaling, enabling full books and hours-long videos as input.\n\nOpen-Weight Releases - Mistral AI, Meta (Llama 3), and newcomer startups released increasingly powerful open base models rivaling 2020-sized GPT-3s, accelerating community fine-tuning and domain adaptation.\n\nReinforcement Learning Advances - DeepMind’s AlphaZero-style “muzero” variants and OpenAI’s large-scale RL at scale (using RLHF and Constitutional AI pipelines) pushed frontier model training pipelines toward continual optimization and post-training alignment.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1432,
        "completionTokensWithoutThoughts": 1432,
        "totalTokens": 1631,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 8423,
      "id": "ee46eda7-d20a-418a-8940-f71ae8e9db49",
      "createdAt": "2025-07-18T14:20:36.863Z",
      "notes": "Covers almost all points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "groq"
        ]
      },
      "response": "2015 - {Deep Learning goes large-scale}\n\nAlphaGo - Google DeepMind created AlphaGo, the first program to beat a professional Go player by blending Monte-Carlo Tree Search with convolutional networks and reinforcement-learning self-play, documented in Nature.\n\nDQN nature revision - DeepMind upgraded its original 2013 DQN with prioritized experience replay, dueling networks and multi-task learning, setting a new bar for Atari game performance while stabilizing value-based RL.\n\nOpenAI founded - Elon Musk, Sam Altman and Greg Brockman formed OpenAI as a non-profit to “democratize” AI, promising to open-source safety-focused breakthroughs in both RL and generative modeling.\n\n2016 - {Generative and RL breakthroughs}\n\nPixelRNN/PixelCNN - Google researchers introduced autoregressive density models for images that predicted one pixel at a time, foreshadowing autoregressive approaches later used in language.\n\nAlphaGo vs Lee Sedol - DeepMind’s agent defeated Lee Sedol 4-1 in Seoul, a global media moment that convinced the public and investors of deep RL’s power.\n\nOpenAI Gym release - OpenAI released its free toolkit of RL environments and benchmark API, letting any researcher swap algorithms against standardized tasks.\n\n2017 - {Attention all you need}\n\nAttention Is All You Need (Transformer) - Google Brain researchers introduced the Transformer architecture (Vaswani et al.), showing self-attention could replace both recurrence and convolution while training orders of magnitude faster.\n\nAlphaGo Zero & AlphaZero - DeepMind’s self-play agents, starting from random weights and no human data, surpassed champion-level performance in Go and Chess by relying purely on tabula-rasa reinforcement learning.\n\nOpenAI Baselines/PPO - OpenAI published baseline implementations of Proximal Policy Optimization (PPO) and made them publicly available, greatly stabilizing policy-gradient RL for robotics and games.\n\n2018 - {Pre-training meets Language}\n\nBERT - Google released Bidirectional Encoder Representations from Transformers (BERT) and demonstrated that large-scale masked-language pre-training massively boosts downstream language tasks.\n\nGPT-1 - OpenAI showed that a 12-layer Transformer language model could learn unsupervised knowledge from BooksCorpus, achieving strong zero-shot results on GLUE benchmarks.\n\nIMPALA - DeepMind scaled distributed actor-critic training to thousands of CPUs, enabling sample-efficient RL at the scale needed for later foundation models.\n\n2019 - {Games, Robotics and Language}\n\nGTP-2 - OpenAI unveiled the 1.5 B parameter GPT-2 and withheld the full model for “safety” reasons, sparking global discussion about control, misuse and the arrival of human-like text generation.\n\nDreamer - DeepMind’s planet-based RL model learned video-game tasks from just latent imagination, showing world-models can rival model-free methods with far less environment interaction.\n\nMuZero - Another DeepMind feat combined model-based planning with self-supervised learning, achieving superhuman play in Atari, Go, Chess and Shogi without human game knowledge.\n\n2020 - {Scale and alignment}\n\nGPT-3 - OpenAI scaled transformers to 175 B parameters with 500 B tokens of web text, demonstrating emergent few-shot, chain-of-thought and in-context learning that kick-started the prompt-engineering era.\n\nCLIP & DALL-E - OpenAI published CLIP (contrastive vision-language pre-training) and DALL-E, proving multi-modal transformers could align images and text, paving the way to later diffusion and chat-image systems.\n\nScaling Laws (Kaplan et al.) - OpenAI researchers showed predictable power-law gains across modalities, giving compute-maximizer labs evidence to spend big on data, parameters and FLOPs.\n\n2021 - {Alignment, RLHF and Code}\n\nInstructGPT - OpenAI used reinforcement learning from human feedback (RLHF) to align GPT-3 with human intent, cutting harmful outputs and becoming the recipe now behind ChatGPT and Claude constitutional-AI tuning.\n\nAlphaCode - DeepMind released a 41 B parameter transformer that solves competitive programming tasks on Codeforces and reached median contestant performance.\n\nGato - DeepMind’s 1.2 B parameter agent showed one model could play Atari, caption images and control real robots, hinting at a path to generalist multi-modal agents.\n\n2022 - {Language models go public}\n\nChatGPT - OpenAI productized InstructGPT into a dialogue interface that gained 100 M users in two months, making conversational LLMs mainstream and igniting a global generative-AI race.\n\nRLAIF - Anthropic introduced Constitutional AI to train Claude with AI-only feedback, showing scalable oversight without thousands of human raters.\n\nChinchilla - DeepMind challenged GPT scale dogma with revised “compute-optimal” scaling laws, prompting labs to train smaller models on more tokens.\n\n2023 - {Claude, GPT-4-turbo, open weights}\n\nGPT-4 - OpenAI released a large multimodal model (image + text) that matches human-level scores on BAR, SAT and Olympiad tasks, while adding system prompts and steerability.\n\nClaude 2 - Anthropic shipped Claude 2 with its 200k “Constitutional” context window, providing an open alternative to OpenAI’s closed API.\n\nLlama 2 - Meta open-sourced 7 B, 13 B and 70 B Llama-2 models with commercial license, setting off a wave of open-source fine-tunes like Vicuna, Alpaca and CodeLlama.\n\n2024 - {Reasoning, agents and regulation}\n\no1 “Strawberry” - OpenAI previewed internal chain-of-thought reasoning models that surpass PhD-level math, coding and science benchmarks by scaling test-time compute and RL search.\n\nGemini 1.5 & Gemini 2.0 - Google DeepMind released a 1 M token context model and follow-on 2.0 version, showing near-perfect recall across long documents and native tool use.\n\nAgent frameworks & EU AI Act - Companies (Anthropic, Microsoft, LangChain etc.) shipped autonomous AI agents, while the EU parliament finalized the world’s first broad AI safety legislation, shaping how future large-scale models must be developed and deployed.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1262,
        "completionTokensWithoutThoughts": 1262,
        "totalTokens": 1461,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 5460,
      "id": "193050f2-6f9b-48f2-aafb-ef06eaca9fab",
      "createdAt": "2025-07-18T14:35:10.475Z",
      "notes": "Covers most points\nWrong format",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 9,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "novita"
        ]
      },
      "response": "2015 - Deep Reinforcement Learning breakthroughs\n\nDQN (Nature paper) - Google DeepMind published “Human-level control through deep reinforcement learning,” showing that a single neural network with Q-learning could master 49 Atari games straight from pixels, establishing deep RL as a practical path to powerful AI agents.\n\nAlphaGo Lee match - Google DeepMind revealed AlphaGo and defeated European champion Fan Hui 5-0, the first time a Go program beat a top professional; the system used a value network and a policy network trained with supervised and reinforcement learning.\n\nA3C - Google DeepMind introduced Asynchronous Advantage Actor-Critic, a lightweight, parallelized RL algorithm that dramatically cut training time and became a standard baseline for continuous control research.\n\n2016 - Supervised-scale generative models emerge\n\nGPT (first) - OpenAI showed that generative pre-training on large text corpora followed by discriminative fine-tuning produced strong natural-language-understanding systems, planting the seed for future language models.\n\nAlphaGo vs Lee Sedol - In Seoul, AlphaGo defeated world champion Lee Sedol 4-1, watched by 200 million people and demonstrating super-human strategic reasoning; the win pushed RL into mainstream consciousness and sparked massive public interest in AI.\n\nWaveNet - Google DeepMind released WaveNet, a deep autoregressive generative model of raw audio that produced natural-sounding speech and inspired parallel work in diffusion-style methods for perceptual domains.\n\n2017 - Attention revolution and AI safety attention\n\nTransformer - Google Brain’s paper “Attention Is All You Need” introduced the transformer architecture with multi-head self-attention, enabling fully parallel training and serving as the critical building block for every modern large language model.\n\nAlphaGo Zero - Google DeepMind removed human data entirely: AlphaGo Zero learned tabula rasa, surpassing the previous version after 40 days and showing that self-play RL plus a unified value/policy network can exceed human knowledge.\n\nProximal Policy Optimization (PPO) - OpenAI published PPO, a stable, easy-to-tune policy-gradient algorithm that became the default for fine-tuning language, robotics, and game agents because of its robustness.\n\n2018 - Language understanding gets contextual\n\nBERT - Google open-sourced BERT, bidirectional encoder representations from transformers trained with masked-language-modeling, setting new records on 11 NLP benchmarks and popularizing “pre-train then fine-tune” across industry.\n\nGPT-1 - OpenAI released the first GPT at 117 M parameters, showing that unsupervised pre-training followed by supervised fine-tuning gives gains on multiple downstream tasks.\n\nAlphaStar - Google DeepMind beat professional StarCraft II players with AlphaMaster league and population-based RL, extending RL to imperfect-information, real-time domains.\n\n2019 - Scaling Up and legibility efforts\n\nGPT-2 - OpenAI scaled GPT to 1.5 B parameters, withheld the full model citing potential misuse, and sparked the “scale matters” debate; GPT-2 generated coherent multi-paragraph text.\n\nMuZero - Google DeepMind unveiled MuZero, learning models of the environment without explicit rules, reaching superhuman play in Atari, Go, Chess, and Shogi while also planning its own future.\n\nOpenAI Five finals - OpenAI defeated the Dota 2 world champions at The International after training agents with PPO and Long Short-Term Memory networks; it showcased large-scale multi-agent RL.\n\n2020 - Language models reach human-like fluency\n\nGPT-3 - OpenAI released GPT-3 with 175 B parameters, demonstrating emergent few-shot abilities via in-context prompting and convincing the public of near-human text generation, propelling interest in foundation models.\n\nRND + curiosity approaches - OpenAI introduced Random Network Distillation, a simple method for intrinsic motivation that accelerated exploration and reinforced principles later used in RLHF pre-training.\n\nChatGPT prototype development - OpenAI internally began combining GPT-3’s language capability with RLHF (reinforcement learning from human feedback) to align model outputs with human intent.\n\n2021 - Alignment and multimodal foundations\n\nInstructGPT - OpenAI published “Fine-Tuning Language Models to Follow Instructions,” using RLHF to turn raw GPT-3 into a helpful assistant and defining the post-training pipeline now used industry-wide.\n\nCodex - OpenAI released Codex, a GPT-derivative fine-tuned on GitHub, powering GitHub Copilot and showing that LLMs + RL can tackle complex programming tasks.\n\nGopher - DeepMind introduced Gopher (280 B parameters) and the Retrieval-Enhanced Transformer, showing scale laws continued and highlighting alignment risks, accelerating safety research.\n\n2022 - The chatbot era starts\n\nChatGPT public launch - OpenAI launched the RLHF-trained, conversational ChatGPT, reaching 100 million users in two months—fastest consumer application growth yet—and caused a global race for similar chatbots.\n\nAlphaTensor - DeepMind discovered faster matrix-multiplication algorithms via a transformer-backed RL agent that treated algorithm discovery as a game, deepening trust in AI-led scientific discovery.\n\nChinchilla scaling laws - DeepMind re-examined compute trade-offs and delivered “Chinchilla optimal” guidance, driving the community to allocate more budget to data and tokens rather than just parameters.\n\n2023 - Multimodal frontiers and open-source storms\n\nGPT-4 - OpenAI released GPT-4, a large multimodal transformer reportedly trained with RLHF, Constitutional AI, and rule-based reward models, excelling on professional exams and establishing state-of-the-art across text, vision, and audio tasks.\n\nClaude 1 & 2 - Anthropic unveiled Claude, emphasizing Constitutional AI where models critique their own outputs against a human-written set of principles, pushing frontier safety alongside capability.\n\nLlama 2 open-source - Meta released Llama 2 up to 70 B parameters for free commercial use, igniting an open-source LLM ecosystem and spawning countless fine-tuned derivatives.\n\nFlashAttention v2 - Together AI and academic collaborators doubled attention speed and cut memory use, making models like LLaMA-65B trainable on a single high-end node and speeding up inference at scale.\n\n2024 - Reasoning-driven scaling\n\nClaude 3 Opus - Anthropic rolled out Claude 3 family with 200 K context and comparable GPT-4 performance, the first publicly available model to beat GPT-4 on key reasoning benchmarks.\n\no1 (Project Strawberry) - OpenAI launched o1-preview, a series of reasoning models trained with large-scale reinforcement learning on chain-of-thought data, significantly outperforming GPT-4 on mathematics, coding, and science.\n\nNemotron-4 340B - NVIDIA released a 340 B parameter open-source model rivaling proprietary systems and detailed efficient training recipes, lowering the barrier to cutting-edge LLMs.\n\nDeepSeek-Coder & Mixture-of-Expert (MoE) surge - Open-source labs such as DeepSeek pushed MoE architectures above 230 B active parameters, using dual-stage pre-training and advanced RL techniques to close the gap with the best closed systems.",
      "reasoning": "",
      "usage": {
        "promptTokens": 190,
        "completionTokens": 1431,
        "completionTokensWithoutThoughts": 1431,
        "totalTokens": 1621,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 120762,
      "id": "27afd440-0791-4b57-8333-7aae45f614f0",
      "createdAt": "2025-07-18T14:37:05.777Z",
      "notes": "Covers most points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "groq"
        ]
      },
      "response": "2015 - {Reinforcement Learning Breakthroughs}\n\nDQN - DeepMind introduced Deep Q-Networks, demonstrating that a single neural network could learn to master many Atari games straight from pixels by fusing Q-learning with convolutional neural networks, establishing deep RL as more than a toy pursuit.\n\nGoogLeNet & ResNet - While not RL, the public release of these very deep image networks showed that scaling depth and parameters unlocked surprising representational power, indirectly encouraging larger RL policies.\n\nOpenAI founded - Sam Altman, Greg Brockman, Elon Musk and others announced OpenAI as a nonprofit counter-weight to Big Tech, explicitly aiming to steer artificial general intelligence to benefit everyone.\n\n2016 - {From Games to Real-World Control}\n\nAlphaGo - Google DeepMind created AlphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search with deep neural networks trained by supervised and reinforcement learning.\n\nTrust-Region Policy Optimization (TRPO) - OpenAI researchers John Schulman et al. published a stable policy-gradient method that monotonically improved expected reward by cleverly constraining each update, becoming work-horse code for many labs.\n\nOpenAI Gym - OpenAI published Gym, a free toolkit with a common interface for RL environments, spurring reproducible research by standardizing benchmark suites like CartPole, MuJoCo robotics tasks and Atari.\n\n2017 - {Attention Is All You Need}\n\nAlphaGo Zero - DeepMind unveiled a pure reinforcement-learning version of AlphaGo that started tabula rasa, discovered classic Go opening theory by self-play, and surpassed the champion-defeating version 100-0, proving self-play alone can reach superhuman performance.\n\nProximal Policy Optimization (PPO) - John Schulman et al. simplified TRPO into a clipped surrogate loss that proved almost as monotonic but vastly easier to implement, becoming the default RL algorithm inside OpenAI and across industry.\n\nAttention Is All You Need - Google researchers released the Transformer paper introducing self-attention for machine translation; the architecture sidestepped recurrence, scaled linearly and turned out to be the missing ingredient for language modeling at scale.\n\n2018 - {Pre-training Meets NLP}\n\nBERT - Google released Bidirectional Encoder Representations from Transformers, showing that plain Masked-Language-Modeling pre-training on a large text corpus could create contextual embeddings that crushed benchmarks after cheap fine-tuning, popularizing bi-directional transformers.\n\nIMPALA - DeepMind scaled distributed actor-critic learning to thousands of CPUs and dozens of Atari environments running in parallel, demonstrating how sheer throughput can match sample efficiency of on-policy methods with off-policy corrections.\n\nOpenAI Five - OpenAI unveiled a PPO-trained system that beat semi-pro teams in the complex MOBA Dota 2, proving modern RL could reason over vast action spaces given sufficient compute, self-play and reward shaping.\n\n2019 - {Toward General Language Agents}\n\nGPT-2 - OpenAI released the 1.5-param GPT-2 model, choosing an incremental staged release to study potential misuse and igniting wide discussion about responsible publication, while demonstrating that simply scaling autoregressive pre-training produced surprisingly coherent long-form text.\n\nAlphaStar - DeepMind's StarCraft II agent reached GrandMaster level for all races using League training, population-based exploration and a pointer network for the combinatorial action space, showcasing multi-agent RL in a real-time-strategy benchmark.\n\nMuZero - DeepMind unified model-based planning with model-free value networks by learning compact environment models on-the-fly; it matched AlphaZero's Go strength and achieved superhuman mastery in Atari without knowing the rules beforehand, heralding model-based RL with imagination.\n\n2020 - {Scaling Laws and Supervised Fine-tuning}\n\nGPT-3 - OpenAI unveiled the 175-param GPT-3, showing few-shot “in-context learning”: prompting alone let the model perform translation, arithmetic and trivia without gradient updates; performance curves hinted at the power-law benefits of further scaling.\n\nLanguage Models as Few-Shot Learner Paper - The release included codex-level prompt engineering and detailed scaling law plots that became internal policy for how to budget more compute for subsequent LLM iterations.\n\nAlphaFold 2 - DeepMind’s breakthrough in protein-structure prediction didn’t use RL for the final step, but relied on attention to replace heuristics, illustrating how self-attention scales to other data modalities beyond language.\n\n2021 - {Aligning Language Models}\n\nCodex & GitHub Copilot - OpenAI fine-tuned GPT-3 on 54 million public GitHub files, creating Codex which powered GitHub Copilot and showed that the same architecture mastered code completion and generation, kicking-off the modern code-assistant market.\n\nScaling Laws for Autoregressive Generative Modeling - Anthropic’s Rausch et al. formalized predictable scaling trends in loss vs parameters, data and compute, guiding the industry to push bigger clusters and longer curriculums rather than algorithmic tricks.\n\nWebGPT - OpenAI combined GPT-3 with a simplified web-search environment trained via RL from comparisons (RLHF), achieving near-human QA on ELI5 by letting the model learn when to query Bing snippets—prefiguring chat search integrations.\n\n2022 - {Chatbot Takeoff}\n\nInstructGPT - OpenAI applied RLHF to GPT-3 to create InstructGPT, demonstrating that human preference ranking data could turn a raw language model into a safer, more helpful assistant that followed instructions and rejected inappropriate queries.\n\nChinchilla - DeepMind re-examined scaling laws and advised the community to equally weight models and training tokens: the now famous “Chinchilla optimal” recipe accelerated data-centric pre-training for LLaMA and later open-source efforts.\n\nChatGPT - OpenAI released ChatGPT (fine-tuned from GPT-3.5 using RLHF plus PPO), sparking viral user adoption and mainstream awareness of aligned conversational AI, registering 100 M users in two months.\n\n2023 - {Anthropic's Safety Focus}\n\nClaude - Anthropic launched Claude, a constitutional AI assistant trained by self-critique to follow an explicit set of harmlessness instructions during RL fine-tuning, popularizing safety-first LLM deployment.\n\nGPT-4 - OpenAI released the multimodal 1.8-trillion-param GPT-4, achieving human-level performance on professional exams, and introduced a system message that constrains behavior—marking the first widely deployed “sparks of AGI” model.\n\nLlama 2 - Meta published the Llama 2 family including 70 B parameter base and chat variants under permissive license, fueling thousands of downstream fine-tunes and democratizing RLHF experimentation by releasing both the model and human-preference datasets.\n\n2024 - {Frontier Reasoning and Open Weights}\n\nClaude 3 - Anthropic debuted Claude 3 Opus, Haiku and Sonnet variants that excel at graduate-level reasoning, coding and image understanding, topping MMLU and HumanEval by intelligently allocating more test-time compute through a mixture-of-experts backbone.\n\nGemini 1.5 Pro & Flash - Google DeepMind extended the Transformer architecture to million-token contexts via sparse Mixture-of-Experts and online distilled attention, enabling long-document QA across entire repositories without retrieval.\n\nMeta Llama 3 - Built with 15 T token curriculum and optimized for Chinchilla efficiency, Meta’s open-weights Llama 3 70 B closes the gap to GPT-4 in many benchmarks while maintaining freely downloadable weights that spur community RLHF/RLAIF research.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1542,
        "completionTokensWithoutThoughts": 1542,
        "totalTokens": 1741,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 8574,
      "id": "d7ac4007-afec-4d0e-8998-96677df8e7e4",
      "createdAt": "2025-07-18T14:40:40.532Z",
      "notes": "Covers most points\nWrong format",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 9,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "deepinfra"
        ]
      },
      "response": "2015 - Deep Reinforcement Learning Breakthroughs\n\nDQN - DeepMind introduced the Deep Q-Network, the first algorithm to master a wide range of Atari 2600 games directly from pixels, combining deep neural networks with Q-learning and replay memory to solve high-dimensional state spaces.\n\nDistributed Deep Reinforcement Learning - DeepMind scaled DQN across 16 workers, showed parallel training dramatically boosts sample efficiency and enabled strong performance on 49 Atari titles in just 30 hours of wall-clock time.\n\nTrust Region Policy Optimization (TRPO) - OpenAI proposed TRPO, which enforces a KL divergence constraint on successive policy updates to guarantee monotonic policy improvement, laying the groundwork for safe and stable on-policy RL.\n\n2016 - Mastery of Go and Unsupervised Representation\n\nAlphaGo - Google DeepMind created AlphaGo, the first program to defeat a professional Go player (Fan Hui 5–0), combining Monte-Carlo tree search with supervised policy and value networks trained on human games and self-play.\n\nOpenAI Gym - OpenAI released Gym, a comprehensive RL benchmark suite and API that standardized environments and evaluation protocols, accelerating public RL research and reproducibility.\n\nWaveNet - DeepMind published WaveNet, a dilated-causal-convolutional generative model that produces more natural human-like speech than parametric or concatenative TTS, demonstrating autoregressive sequence modeling in audio.\n\n2017 - Transformer & Attention Is All You Need\n\nAttention Is All You Need - Google researchers published the Transformer architecture, replacing recurrent or convolutional layers entirely with multi-head self-attention for unprecedented parallelization and long-range dependency modeling in sequence-to-sequence tasks.\n\nAlphaGo Zero - DeepMind introduced AlphaGo Zero, which learns tabula rasa without human game data, using pure reinforcement learning with self-play and a unified policy-value network, surpassing the original AlphaGo 100-0.\n\nProximal Policy Optimization (PPO) - OpenAI presented PPO, a clipped-surrogate objective that simplifies TRPO into a first-order method with superior sample efficiency and robustness, making general RL more accessible.\n\n2018 - Self-Supervised & Transferable Language Models\n\nBERT - Google released BIDirectional Encoder Representations from Transformers, pre-trained on masked-language-modeling and next-sentence-prediction objectives, instantly setting new records on eleven NLP benchmarks through fine-tuning.\n\nGPT - OpenAI introduced the first Generative Pre-trained Transformer, a 117 M-parameter causal language model trained on BooksCorpus that demonstrates unsupervised pre-training plus supervised fine-tuning can transfer broadly to downstream tasks.\n\nELMo - AllenNLP popularized Embeddings from Language Models, deep bi-LM representations that add contextual, character-level information and significantly boost performance on tasks like SQuAD, signaling the end of static word vectors.\n\n2019 - Scaling, Game Mastery & Policy Transfer\n\nGPT-2 - OpenAI built a 1.5 B-parameter language model whose impressive generation quality led to staged release debates; it demonstrated emergent zero-shot capabilities and the power of large-scale unsupervised pre-training.\n\nAlphaStar - DeepMind achieved Grandmaster level in the full game of StarCraft II using multi-agent RL and league training, combining policy distillation and human-replay initialization to manage long-horizon strategic reasoning.\n\nMuZero - DeepMind unified model-based RL by learning an internal latent model of environment dynamics without privileged knowledge of rules; it matched AlphaZero’s performance in Go, chess and shogi while mastering 57 Atari games.\n\n2020 - Scale, Language, Vision & Diffusion\n\nGPT-3 - OpenAI scaled transformers to 175 B parameters and demonstrated few-, one- and zero-shot prompting, dramatically reducing the need for task-specific fine-tuning and showcasing emergent reasoning in arithmetic and translation.\n\nVision Transformer (ViT) - Google showed that a simple transformer applied directly to sequences of image patches achieves state-of-the-art on ImageNet when pre-trained on large datasets, extending the transformer success to computer vision.\n\nDALL·E & CLIP - OpenAI released DALL·E (12 B-parameter transformer) generating images from text prompts, and CLIP, a dual-encoder model aligning vision and language for zero-shot classification across arbitrary image classes.\n\nDDPM - Google Brain first made diffusion probabilistic models competitive on image generation, introducing a denoising objective and Gaussian noise scheduling that would later power DALL·E 2, Stable Diffusion and Imagen.\n\n2021 - Instruction Tuning, Alignment & Generative APIs\n\nInstructGPT - OpenAI applied reinforcement learning from human feedback (RLHF) to fine-tune GPT-3 for instruction following, showing the paradigm of supervised SFT + PPO + KL control to align models with user intent and boost quality.\n\nCodex & GitHub Copilot - OpenAI distilled a 12 B-parameter model from GPT trained on 54 M GitHub files, enabling an AI pair-programmer in VS Code that autocompletes multiline code from natural language comments.\n\nGato - DeepMind introduced a single 1.2 B-parameter transformer agent that unifies vision, language and action, controlling a robot arm, playing Atari, captioning images and chatting with the same network weights.\n\nFlamingo - DeepMind developed a frozen-image-encoder + LM few-shot visual interface that answers questions about new images by in-context learning, bridging language and vision with under 80 B total training shots.\n\nWebGPT - OpenAI enhanced GPT-3 to browse, search and cite the web, combining LM generation with retrieval to provide factual, grounded answers akin to internet-augmented reasoning.\n\n2022 - RLHF Mainstream & Chatbots Emerge\n\nChatGPT - OpenAI publicly launched ChatGPT, a fine-tuned version of GPT-3.5 optimized with RLHF for conversational assistance, rapidly passing 100 M users in two months and igniting global interest in large language models.\n\nScaling Laws for Neural Language Models - DeepMind showed Chinchilla (70 B tokens-optimal model) outperforms larger counterparts by matching model size and data size, revising the compute frontier and informing efficient training strategies.\n\nStable Diffusion - Stability AI released an open-source latent diffusion model (LDM) that brought high-quality, fast text-to-image synthesis to consumer GPUs, catalyzing a creative-AI movement.\n\nGalactica - Meta unveiled a large language model for scientific literature that answers questions, summarizes papers and writes LaTeX, highlighting both promise and pitfalls of domain-specific LLMs.\n\n2023 - Multimodal Reasoning, Safety Debates & Open Weights\n\nGPT-4 - OpenAI launched GPT-4, a multimodal transformer supporting both text and vision inputs that scores high on human exams (BAR,SAT,GRE), introduces steerability via system messages and sets new safety benchmarks following red-teaming efforts.\n\nClaude 2 - Anthropic released Claude 2, a constitutional-AI-assisted model trained with RL from AI feedback, emphasizing safety, harmlessness and helpfulness while allowing 100 K-token context windows for long-document processing.\n\nLlama 2 & Llama 3 - Meta open-sourced Llama 2 (7-70 B) with commercial license and later Llama 3, providing the community transparent weights and energizing research into instruction tuning, alignment and open-source LLM ecosystems.\n\nDALL·E 3 - OpenAI integrated GPT-4 prompt rewriting into the diffusion pipeline, letting users describe complex scenes with better nuance and coherence, and allowing safe editing of existing images.\n\nGemini 1 & 1.5 - Google DeepMind announced Gemini, a natively multimodal family (Nano, Pro, Ultra) later scaled to 1.5’s 10M-token context window and mixture-of-experts design, achieving new SOTA on MMLU and multimodal reasoning tasks.\n\nBard / Gemini Advanced - Google rebranded Bard into Gemini line, deploying the web-browsing assistant globally and launching a paid Gemini-Advanced tier based on Ultra model for competitive consumer AI access.\n\n2024 - Agents, World Models & Robot Reasoning\n\nOpenAI o1 (\"Strawberry\") - OpenAI previewed a reasoning model that spends extra compute on internal chain-of-thought deliberation, achieving PhD-level performance in physics, chemistry and math and bringing test-time scaling to the forefront.\n\nClaude 3 / 3.5 Sonnet - Anthropic released Claude 3 family and later 3.5 Sonnet with state-of-the-art coding abilities, 200 K-token contexts and built-in tool-calling for end-to-end agentic workflows on the web.\n\nSora - OpenAI revealed Sora, a diffusion-transformer video model able to generate minutes-long photorealistic clips from text or images, showcasing emergent physics simulation and long-range spatiotemporal coherence.\n\nGenie - DeepMind introduced Genie, a 11 B-parameter world-model baseline trained on 200 K hours of unlabelled internet videos, generating interactive 2D environments and demonstrating scalable foundation models for controllable virtual worlds.\n\nRT-2 & Vision-Language-Action models - Google DeepMind fused PaLM-E language-vision with robotic control, creating RT-2 that directly outputs low-level robot actions from web-scale pre-training, proving generalization from internet knowledge to manipulation tasks.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1899,
        "completionTokensWithoutThoughts": 1899,
        "totalTokens": 2098,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 25681,
      "id": "2b467db7-8cb0-4c4a-9722-6f672aa8c914",
      "createdAt": "2025-07-18T14:40:57.640Z",
      "notes": "Covers most points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 9,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "novita"
        ]
      },
      "response": "2015 - DeepMind masters Atari with end-to-end RL and opens the era of attention-based language representation.\n\nDQN Nature Paper - DeepMind introduced the Deep Q-Network (DQN), showing that a single neural network could learn to play dozens of Atari 2600 games at super-human level by combining Q-learning with convolutional vision models, sparking the modern deep-RL wave.\n\nshow, attend and tell - Google’s Kelvin Xu et al. popularized attention mechanisms with image-captioning, providing the architectural seed for the transformers that will later power language models.\n\nTorch open-sourced - Facebook AI Research (FAIR) released Torch and the nngraph framework, giving researchers the efficient GPU-optimized tools needed to iterate on increasingly large models.\n\n2016 - Reinforcement learning defeats the world champion of Go and new policy-gradient methods tame continuous control.\n\nAlphaGo - Google DeepMind created AlphaGo, the first program to beat a top Go professional, Lee Sedol, by blending Monte-Carlo tree search with value and policy networks trained by supervised and reinforcement learning.\n\nGAE & TRPO - John Schulman (then at OpenAI) formulated Generalized Advantage Estimation and Trust-Region Policy Optimization, stabilizing policy-gradient RL and enabling reliable learning on high-dimensional continuous tasks.\n\nOpenAI Gym - OpenAI launched Gym, an open benchmark suite and API for RL environments, instantly standardizing research on Atari, MuJoCo and classic control tasks.\n\n2017 - Transformers shift the field from recurrent to self-attention, and self-play births super-human chess and shogi engines.\n\nAttention Is All You Need - Google Brain’s Ashish Vaswani et al. introduced the Transformer architecture, replacing recurrence with self-attention to enable unprecedented parallel training and set the blueprint for modern LLMs.\n\nAlphaGo Zero & AlphaZero - DeepMind removed human data for Go, chess and shogi, training via pure self-play with a single network and Monte-Carlo tree search, proving that tabula-rasa RL can surpass all human knowledge.\n\nProximal Policy Optimization (PPO) - OpenAI’s Schulman et al. released PPO, a clipped-surrogate policy-gradient algorithm that became the default choice for robust RL in robotics, games and language fine-tuning.\n\n2018 - Pre-training + fine-tuning becomes the new NLP paradigm, and curiosity-driven exploration begins to scale.\n\nBERT - Google open-sourced Bidirectional Encoder Representations from Transformers, demonstrating that unsupervised pre-training on large text followed by task-specific fine-tuning sets new records across almost every NLP benchmark.\n\nGTP-1 - OpenAI published “Improving Language Understanding by Generative Pre-Training,” unveiling the first large Transformer language model (117 M parameters) and showing that scaling self-attention plus next-token prediction yields broad few-shot abilities.\n\nIMPALA - DeepMind delivered a scalable, asynchronous actor-critic system that trains on 30 billion frames across hundreds of environments using the now-standard V-trace off-policy correction.\n\n2019 - Language models expand toward 100 B parameters, RL agents conquer StarCraft II and MuJoCo robotics, and OpenAI coins RLHF.\n\nGPT-2 - OpenAI trained a 1.5B-parameter Transformer, withheld the full model for “safety” reasons, then released it and demonstrated unsupervised zero-shot transfer while igniting an industry conversation on scale, distribution and misuse.\n\nAlphaStar - DeepMind produced league-training plus human imitation to reach Grandmaster level on the full StarCraft II ladder, mastering partial observability and long-term strategy in a mainstream e-sport.\n\nRLHF formulation - Building on earlier preference-learning work, OpenAI and Paul Christiano formalized “RL from Human Feedback,” creating a recipe that will later power InstructGPT and ChatGPT’s alignment.\n\n2020 - Scaling laws are codified, GPT-3 delivers emergent few-shot learning and MuZero masters multiple environments with no human rules.\n\nGPT-3 - OpenAI scaled Transformers to 175 B parameters and published the paper “Language Models are Few-Shot Learners,” showing that large enough networks can meta-learn in-context without weight updates.\n\nMuZero - DeepMind introduced MuZero, which learns an environment model entirely by latent imagination, achieving super-human play in Atari, chess, shogi and Go without ever receiving rules or exact dynamics.\n\nScaling Laws - OpenAI researchers Kaplan, McCandlish and Henighan published empirical scaling laws relating model size, data, compute and performance, giving clear guidance to steer billion-dollar training runs in the following years.\n\n2021 - Encoder-decoder contrasts spawn CLIP and Flamingo vision-language models, all while super-giant dense and sparse compute experiments reach the 500B–1T parameter regime.\n\nMegatron-Turing NLG 530B - NVIDIA and Microsoft jointly trained a dense 530 B parameter Transformer on 270 B tokens of English text, setting new few-shot accuracy records on SuperGLUE and MMLU.\n\nLaMDA & PaLM - Google revealed Language Model for Dialogue Applications and Pathways Language Model (540 B), spotlighting breakthroughs in conversational factuality, retrieval-augmented training and multilingual reasoning.\n\nCLIP & DALL-E - OpenAI released CLIP, a vision-language contrastive pre-training method, and DALL-E, a 12 B parameter transformer that composes images from text, founding the generative-art movement.\n\nInstructGPT - Using RLHF, OpenAI fine-tuned GPT-3 to follow human instructions, cutting hallucinations and aligning language models with user intent via reward-modeling + PPO.\n\n2022 - Aligning large models becomes central: RLHF, Constitutional AI and browser-preview transformers lead to ChatGPT’s viral debut.\n\nChatGPT - OpenAI fine-tuned GPT-3.5 with RLHF and launched a conversational web demo. It reached 100 M users in two months, placing generative AI on every CEO’s roadmap.\n\nChinchilla - DeepMind re-examined scaling laws and established the “Chinchilla-optimal” prescription: equal investment in model size and data, spurring more data-efficient trillion-token corpora.\n\nSparrow & Constitutional AI - DeepMind introduced dialogue agents constrained by human preferences, while Anthropic used “Constitutional AI” (self-critique + RL from AI feedback) to produce helpful, harmless, honest assistants.\n\nSparrow - DeepMind built a dialogue agent that answers questions and searches the web while reducing harmful or false responses, incorporating adversarial probing and human-rule constraints.\n\nStable Diffusion - Stability AI open-sourced a diffusion image model with 1 B parameters that democratized high-resolution text-to-image generation on consumer GPUs.\n\n2023 - GPT-4 raises the bar for multimodal reasoning and Claude 2 enters the arena, while open-source challengers (Llama 2, Mistral), tool-using agents and autonomous coding gain steam.\n\nGPT-4 - OpenAI released a large multimodal Transformer capable of vision inputs, scoring among the top 10 % of human test-takers on the Uniform Bar Exam. It was trained with massive RLHF and red-teaming to refuse unsafe requests.\n\nClaude 2 - Anthropic offered a 100 k-token-context constitutional‐AI assistant, emphasizing alignment research and long-form document processing critical for enterprises.\n\nLlama 2 - Meta open-sourced a family of 7 B – 70 B Llama-2 models for free commercial use, catalyzing a global bloom of open-source fine-tunes (Code Llama, Vicuna, Orca) and on-device inference.\n\nGemini - Google DeepMind introduced Gemini 1.0 across Ultra, Pro and Nano sizes, claiming state-of-the-art on MMLU and multimodal benchmarks, and later Gemini 1.5 with up-to 1 M token context.\n\n2024 - Multimodal, long-context and reasoning agents become commodities, while frontier labs race toward GPT-5, Claude-3-scale Opus, and algorithmic innovations in test-time compute.\n\nClaude 3 Family - Anthropic released Claude 3 Haiku, Sonnet and Opus, pushing long-context reasoning to 200 k tokens and premium performance across MMLU and GPQA, proving safety can scale with capability.\n\nGPT-4o (Omni) - OpenAI shipped a natively multimodal Transformer that ingests audio, vision, and text in real time, slashing latency and unifying modalities in a single end-to-end model.\n\nLlama 3 - Meta open-sourced 8 B and 70 B base / instruction models trained on 15 T tokens, matching or beating proprietary rivals on key benchmarks, and teased a dense 400 B+ successor in training.\n\nReasoning with AlphaGo-style tree search - Teams at Google DeepMind, Microsoft and OpenAI embed latent search (e.g., AlphaCode 2, o1-preview) during inference, showing that RL-style test-time exploration can surpass larger static models on math and code.\n\nThese ten years transformed AI research from narrowly super-human game players and 100 M parameter text RNNs to ubiquitous, half-trillion-parameter multimodal assistants capable of reasoning over entire books, writing code and holding open-domain conversations.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1875,
        "completionTokensWithoutThoughts": 1875,
        "totalTokens": 2074,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 25944,
      "id": "cd7c8845-9fb5-4857-8656-d3ea9fdcf64d",
      "createdAt": "2025-07-18T14:40:57.903Z",
      "notes": "Covers most points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "together"
        ]
      },
      "response": "2015 - DQN, Attention, and OpenAI\n\nDQN - Google DeepMind published “Human-level control through deep reinforcement learning,” proving that a single neural network (a deep Q-network trained end-to-end with experience replay and target networks) could master dozens of Atari 2600 games directly from raw pixels, launching deep RL into mainstream AI research.\n\nAttention mechanism - Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio introduced additive attention (“Neural Machine Translation by Jointly Learning to Align and Translate”), showing that soft alignments learned jointly with an encoder-decoder RNN let the model focus on relevant parts of the input.  This concept becomes a core ingredient in every later Transformer.\n\nOpenAI founded - Sam Altman, Greg Brockman, Ilya Sutskever, Elon Musk and others announce OpenAI, a non-profit dedicated to ensuring that AGI benefits all of humanity, providing money and talent that will later produce GPT series and CLIP.\n\n2016 - AlphaGo and GPUs for learning\n\nAlphaGo - Google DeepMind created AlphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCTS) with deep neural networks trained by supervised and reinforcement learning.\n\n2017 - Attention is all you need\n\nThe Transformer - Google Brain’s “Attention Is All You Need” (Vaswani et al.) replaced recurrent and convolutional sequence models with multi-head self-attention and positional encodings, dramatically improving parallelization and setting the architecture for future LLMs.  Soon after, the same authors trained the first large Transformer encoder-decoder on English-German translation, surpassing state-of-the-art BLEU on WMT 2014.\n\n2018 - BERT and the dawn of contextual word models\n\nBERT - Google AI released BERT (Bidirectional Encoder Representations from Transformers).  Jacob Devlin and team pre-trained a bidirectional Transformer encoder on BooksCorpus+Wikipedia with masked-language-modeling and next-sentence prediction, then fine-tuned it on 11 NLP benchmarks, igniting the “pre-train then fine-tune” paradigm.\n\nGPT-1 - OpenAI published “Improving Language Understanding by Generative Pre-Training.”  Alec Radford et al. showed that 117 million-parameter Transformer decoder trained to predict next tokens on BookCorpus could transfer to many downstream tasks with minimal fine-tuning, an early sign that simple autoregressive pre-training yields broadly useful representations.\n\n2019 - More parameters and single-agent excellence\n\nGPT-2 - OpenAI scaled to 1.5 billion parameters on 40 GB of internet text.  The model generated coherent multi-paragraph samples, prompting the headline-grabbing staged release policy (initially withholding the full model) and popularizing concerns about emergent capabilities.\n\nOpenAI Five - OpenAI defeated the DOTA 2 world champions OG in a best-of-three exhibition match.  The system used Proximal Policy Optimization (PPO) on 180 years of self-play per day across distributed GPUs, demonstrating that pure RL can master a complex collaborative/competitive game.\n\n2020 - GPT-3, scaling laws, and multimodal RL\n\nGPT-3 - OpenAI released a 175 billion-parameter Transformer that showed dramatic few-shot learning: prompts alone could elicit translation, arithmetic, and code generation, transitioning the field from “fine-tune” to “prompt.”  Blog/demo “Language Models are Few-Shot Learners” broke into public consciousness and seeded a wave of API startups.\n\nOpenAI Scaling Laws - Tom Brown, Jared Kaplan and others published empirical scaling laws showing that language-model loss falls predictably with parameter count, dataset size, and compute, guiding later investments in bigger clusters.\n\nVision-Language RL - DeepMind debuted MuZero (Schrittwieser et al.) which learns to play board and Atari games with no knowledge of rules or dynamics, performing model-based planning inside learned latent representations; conceptually links world models to RL agents.\n\n2021 - Claude, Constitutional RL and giant models\n\nConstitutional RL - Anthropic (Dario Amodei, Jared Kaplan, Deb Raji, et al.) outlined “Constitutional AI,” a recipe for training helpful and harmless assistants via self-critique and RL from AI (RLAIF), iterated with human-supplied constitutions; forms the backbone of Claude’s alignment training.\n\nMegatron-Turing NLG - Nvidia and Microsoft trained a 530-billion-parameter Transformer on the Selene supercomputer using 3D parallelism, demonstrating efficient training of 100B+ dense models.\n\nInstructGPT - OpenAI replaced raw GPT-3’s next-token objective with reinforcement learning from human feedback (RLHF): supply rankings, fine-tune via PPO, then distill.  Surpasses vanilla GPT-3 on human evaluations despite having 100× fewer parameters during RL.\n\n2022 - ChatGPT and the consumer LLM shift\n\nChatGPT - OpenAI turned InstructGPT into a dialogue-optimized system deployed free to the web.  Within two months it gained 100 million users, turning conversational AI into a daily consumer product and catalyzing Big Tech’s product roadmaps.\n\nPaLM - Google released Pathways Language Model (540 B params) trained on the new Pathways system, showing multilingual few-shot reasoning and code, paralleling GPT-3’s capabilities from a major search provider.\n\nChinchilla - DeepMind’s “Training Compute-Optimal Large Language Models” empirically reset scaling guidance: equal weight to parameter count and data (tokens) upends the prior “bigger always” regime; becomes a recipe for GPT-4 and Claude scaling budgets.\n\n2023 - GPT-4, Claude 2 and new alignment techniques\n\nGPT-4 - OpenAI shipped a multimodal Transformer (up to ~1.8 T sparse + dense weights) trained with large-scale RLHF, rule-based reward models, and red-team alignment sweeps, passing professional exams and coding interviews worldwide.\n\nClaude 2 - Anthropic publicly released Claude 2 with 100k-token context window, constitutional pre- and post-training, and a “trade-off frontier” report on harmlessness versus helpfulness, demonstrating RLHF at Anthropic-scale.\n\nRetNet & Hyena alternatives - Researchers at Microsoft (Sun et al.) and CMU/Hazy Research (Poli et al.) proposed efficient sub-quadratic sequence models aiming to challenge full-softmax Transformers; the debate keeps architectural innovation alive.\n\n2024 - Rise of frontier reasoning models and open smalls\n\no1 / Strawberry - OpenAI introduced “o1,” a training stack re-budgeting compute at inference-time via chain-of-thought search and reinforcement learning, enabling PhD-level STEM problem solving.  Implicitly extends DeepMind’s AlphaGo/Zero search ideas to natural language.\n\nClaude 3 family - Anthropic released Claude 3 Haiku/Sonnet/Opus, demonstrating near-parity with GPT-4 on key benchmarks and outperforming on long-context recall and safety red-team scores using improved constitutional taxonomies.\n\nLlama 3 and smaller opens - Meta AI open-sourced Llama 3 (8 B and 70 B variants) trained on 15 T tokens, surpassing closed competitors of the same size and fueling the rapid open-source application ecosystem.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1501,
        "completionTokensWithoutThoughts": 1501,
        "totalTokens": 1700,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 40070,
      "id": "71a58760-2b76-4d81-8804-4837617e38ed",
      "createdAt": "2025-07-18T14:41:12.029Z",
      "notes": "Covers major points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 10,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "deepinfra"
        ]
      },
      "response": "2015 - DeepMind masters Atari games with DQN\n\nDeep Q-Networks (DQN) - Google DeepMind published a landmark paper showing that a single neural architecture could learn to play dozens of Atari games straight from pixels, breaking the barrier that required hand-crafted features. With experience replay and target networks, an agent reached human-level performance on 49 games including Breakout and Pong.\n\nAlphaGo vs Lee Sedol - DeepMind’s AlphaGo defeated world champion Lee Sedol 4-1 in Seoul, demonstrating super-human Go play by bootstrapping from 160,000 expert games then improving through self-play.\n\nREINFORCE and Policy Gradients - Continued refinement of Monte-Carlo policy gradient methods renewed interest in REINFORCE, Advantage Actor-Critic, and Trust Region Policy Optimization foundations that would lead to PPO in 2017.\n\n2016 - AlphaGo Zero & Unsupervised NMT\n\nAlphaGo Zero - DeepMind released a purely self-supervised AlphaGo that learned tabula rasa, surpassing its predecessors within 40 days by playing 4.9 million games against itself, invalidating the need for human expert data and showing the possibility of self-play-driven discovery (inspired AGI research greatly).\n\nWaveNet - DeepMind released a deep autoregressive audio generation model that produced shockingly human-sounding speech from raw waveform, influencing the design of Transformer-based language models in 2017-2019.\n\nAttention Is All You Need attention - Google Brain & University of Toronto researchers published theVision-is-TO-attention result that for attention mechanism suffices for machine translation, laying transformer groundwork for LLMs in 2018-2023.\n\nPolicy Gradient Methods - Work on Proximal Policy Optimization (PPO) and Advantage Actor-Critic (A2C) continued. These algorithms later became staples used to train RLHF preferences in GPT models.\n\n2017 - The Transformer\n\nTransformer - Google Brain’s Attention-is-All-You-Need paper introduced the Transformer architecture, eliminating the need for recurrent or convolutional nets. The encoder-decoder model enabled parallel training and state-of-the-art translation on WMT.\n\nAlphaZero - DeepMind generalized the self-play framework from Go to chess and shogi, achieving world-champion level in both within 24 hours of self-training without human gameplay data—broadening RL self-play across domains.\n\nProximal Policy Optimization (PPO) - OpenAI released PPO, a simple but robust policy-gradient algorithm that stabilizes training via clipping of SAMTRIDGE proxies, later used by OpenAI & Anthropic to align LLMs via RLHF.\n\n2018 - Big Models & Pre-training\n\nBERT - Google AI released BIDirectional Encoder Representations from Transformers, the first large-scale bidirectional pre-trained transformer. By masking 15 % of tokens and predicting them from both left & right context, BERT reset state-of-the-art in 11 NLU tasks including GLUE and SQuAD v1.1.\n\nAlphaStar - DeepMind’s agent reached Grandmaster level in StarCraft II with a multi-agent population trained via self-play—a milestone for real-time strategy games and sparse-reward RL.\n\nGPT - OpenAI scaled the transformer decoder and released GPT-1 (117 M params), demonstrating that unsupervised pre-training on a large web corpus followed by supervised fine-tuning (semi-supervised transfer) works at scale.\n\nCurriculum RL - Uber introduced domain randomization and automatic curricular learning to train agents like Solving Rubik’s Cube by awareness—precursors to LEGO-style curricula now used to teach LLMs.\n\n2019 - GPT-2 Scare & Constitutional AI\n\nGPT-2 - OpenAI stunned the community by withholding 1.5 B-parameter GPT-2, fearing misuse. Its generative ability and zero-shot transfer revealed scaling laws: bigger models → emergent capabilities like multi-task NLP, machine translation, and summarization without fine-tuning.\n\nRobust Self-Play (OpenAI Five) - OpenAI’s five-hero team defeated world-champion Dota 2 players by training billions of frames of self-play matches and concurrently-executed PPO on 256 GPUs, showcasing scalable multi-agent RL.\n\nCIFAR 10 w/ Adversarial Training & Semi-Supervised Learning - MixMatch and UDA demonstrated the power of consistency, helping reinforce that multiple modalities incl. RL—later used in LLM safety filters (e.g., RLHF fine-tuning).\n\n2020 - GPT-3 & RLHF Inception\n\nGPT-3 - OpenAI released GPT-3 (175 B parameters), exhibiting meta-learning & few-shot “prompting”. Its scale enabled arithmetic, code generation, and question-answering with mere prompts, birthing the term “foundation model” as coined by Stanford HAI.\n\nInstructGPT/RLHF - OpenAI experimented on aligning GPT-3 behavior via fine-tuning human preferences—began the chain of RLHF (RL via Proximal Policy Optimization on human-rating datasets) that define GPT, Claude, and all following LLM behavior tuning.\n\nCLIP & DALL-E - OpenAI released CLIP which aligned textual and visual embeddings via contrastive learning, and DALL-E generated images from text. Vision-language pre-training laid groundwork for multimodal GPT.\n\nMuZero - DeepMind introduced MuZero, generalizing AlphaZero by learning a latent dynamics model purely from planning—that is, no access to game dynamics, thus certifying model-based RL can succeed in high-dimensional state spaces.\n\n2021 - Scaling Laws & Constitutional AI\n\nChinchilla/PaLM/Scaling Laws - DeepMind’s “Chinchilla” paper empirically revisited compute trade-offs, proving that data & params growth both scale performance—reframed how to optimally train LLMs as seen in 2022-2024.\n\nRLHF Milestone - Anthropic introduced Constitutional AI, training Claude to adhere to explicit human rules via RL from AI feedback (RLAIF) and a two-stage RLHF process: helpful vs harmless preferences.\n\nCodex - OpenAI’s GPT descendent trained on GitHub code corpora enabled GitHub Copilot, showing models can write entire functions from docstrings—another empirical rung in emergent LLM capability.\n\nAlphaCode & Jurassic-1 - DeepMind unveiled AlphaCode achieving human median in competitive coding contests; AI21 Labs released 178 B Jurassic-1, pushing commercial LLM frontiers.\n\n2022 - GPT Debuts & RLHF Aligns ChatGPT\n\nRLHF ChatGPT (Instruct GPT-3.5) - On 30 Nov 2022 OpenAI deployed ChatGPT, powered by RLHF-optimized GPT-3.5. A user-friendly chat interface reached 100 M users in two months, catalyzing global adoption and “ prompting ” culture highlights the need for alignment engineering.\n\nPaLM (540 B) - Google announced the 540 billion parameter PaLM, demonstrating breakthroughs on BIG-Bench including multilingual translation, reasoning tasks, and math word problems—underscoring the effectiveness of scaling & scaling-law prescriptions.\n\nFlan-T5 & Self-Play RLHF - Google’s Flan-prompting instruction-finetuned T5 and high-quality datasets (Flan-v2) became reference gold standards; concurrent Anthropic continued RLHF with constitutional chains-of-thought to reduce hallucinations and support RL from AI feedback experiments.\n\nStable Diffusion - Stability AI open-sourced the latent diffusion text-to-image model in August 2022—they share a lineage with GPT autoregressive training via denoising diffusion objective boosting multimodal LLM research workflows.\n\n2023 - The Multimodal RLHF Era & Claude/GPT-4\n\nGPT-4 - OpenAI released GPT-4 (8 × 220 B “Mixture-of-Experts”) in March 2023 featuring: multimodal vision + text reasoning, correspondence grounding, beats humans on Bar Exam, and alignment-quality via extensive RLHF-filtered datasets red-teaming.\n\nClaude-2 - Anthropic launched Claude-2 (12 k-token CoT prompting friend), improving safety via Constitutional AI + RL from AI feedback (RLAIF), safer edge-case jailbreak handling, and up to 200 k max token window for long-form inputs.\n\nVoyager and MineAgent - Nvidia released Voyager, a GPT-4-driven agent that prepends skill libraries and writes its own exploration code in Minecraft via API prompts—illustrates LLM-as-policy or “Ctrl-Cmd ▶️ Next Level”.\n\nRLHF Scaling Spree - Anthropic introduced Constitutional Chain-of-Thought, Google with PaLM-2 (RLHF & fine-tuning), Meta Llama-2 fine-tuned w/ helpful harmless human red-teaming, and OpenAI’s Superalignment research anchoring “the next RLHF wave”.\n\n2024 - Emergent Agentic Reasoning & AI safety\n\nClaude 3 / GPT-4o / Gemini Ultra multimodal agents - Anthropic Claude 3 Opus (200 k ctx), Google Gemini Ultra (1 M token), and OpenAI GPT-4o all converged on tool-calling, long-horizon reasoning, and chain-of-thought self-critique—practicalizing Retrieval-Augmented Generation (RAG) and agent frameworks (LangChain, CrewAI).\n\nOpenAI O1 & Internal CoT Reasoning - OpenAI revealed “o1-preview”, a model trained to generate private chain-of-thought ahead of answers using reinforcement learning-like self-scrutiny, hence “reasoning-tuned” rather than next-token predict “tuned” effectively bridging classical RL planning with large-scale LLMs.\n\nRLHF 2.0: Assistants & Self-Critique - Concurrent work from DeepMind, Anthropic, & OpenAI introduced iterative RLHF on ASCL (Assistant Self-Critique Looping) tutorials, integrating safe grading networks, constitutional reviews, and tool-use audit-logs proving that RL can handle open-ended safety evaluations.\n\nAgentic Ecosystems - timely releases from Hugging Face (transformers), LangChain, CrewAI, AutoGen, LlamaIndex denote an RL-driven agent future where LLMs, RLHF basics, and Web-augmented tool-use swiftly merged—marking consolidation from pure RL breakthroughs (2015-2020) to LLM-centric RLHF post-2020, now culminating in general reasoning agents.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 2126,
        "completionTokensWithoutThoughts": 2126,
        "totalTokens": 2325,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 35313,
      "id": "b52a05c3-3561-4146-9c34-f4c410e85733",
      "createdAt": "2025-07-18T14:47:23.433Z",
      "notes": "Cover all the points!",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 9,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "together"
        ]
      },
      "response": "2015 - Deep Reinforcement Learning Starts Scaling\n\nDQN - Google DeepMind introduced the Deep Q-Network, published in Nature in Feb 2015, showing that a single neural network with Q-learning can reach super-human scores on many Atari 2600 games directly from raw pixels.\n\nTensorFlow - Google released its open-source machine-learning framework, providing the distributed tooling that later enables training of billion-parameter models.\n\n2016 - Super-human Control and Unsupervised Language Insight\n\nAlphaGo - Google DeepMind created AlphaGo, the first program to defeat a top Go professional (Lee Sedol) by blending Monte-Carlo tree search with policy and value networks trained by supervised learning and self-play reinforcement learning.\n\nSeq2Seq with Attention - Google Brain published “Neural Machine Translation by Jointly Learning to Align and Translate”, popularizing attention mechanisms that become core to every later large language model.\n\nUnsupervised Sentence Representations - OpenAI showed that LSTM language models alone learn transferable sentence features, hinting at the future power of unsupervised pre-training.\n\n2017 - Attention Is All You Need\n\nTransformer - Google Brain’s “Attention Is All You Need” paper, led by Ashish Vaswani and Jakob Uszkoreit, introduced the self-attention-only architecture that will replace recurrent networks as the foundation for large-scale language models.\n\nPPO - OpenAI released Proximal Policy Optimization, an efficient policy-gradient algorithm that stabilizes RL training on continuous and discrete control benchmarks and later drives RLHF.\n\n2018 - Pre-trained Language Models Emerge\n\nGPT-1 - OpenAI showed that unsupervised pre-training on BooksCorpus followed by supervised fine-tuning (the semi-supervised GPT recipe) improves downstream NLP tasks, demonstrating generative pre-training for language.\n\nBERT - Google released Bidirectional Encoder Representations from Transformers, proving that large bidirectional masked-language-model pre-training gives state-of-the-art results on GLUE, SQuAD and other benchmarks.\n\nIMPALA - DeepMind introduced Importance-Weighted Actor-Learner Architectures, scaling RL across thousands of environments and CPUs, foreshadowing massive-data training pipelines.\n\n2019 - Large-scale Self-supervised Learning Takes Off\n\nGPT-2 - OpenAI released a 1.5-billion-parameter generative model whose performance led to staged disclosure; its Story-Gen demo and eventual full release showcased emerging few-shot learning abilities.\n\nT5 - Google introduced the “Text-to-Text Transfer Transformer,” unifying every NLP task under a single seq2seq Transformer framework with massive C4 pre-training.\n\nTLSM - Facebook and the UW explored two-stage transfer: unsupervised pre-training on massive text followed by supervised fine-tuning, a pattern now standard in LLM pipelines.\n\n2020 - Trillion-token Models and the Scaling Hypothesis\n\nGPT-3 - OpenAI presented a 175-billion-parameter autoregressive language model demonstrating startling in-context learning, meta-learning, and few-shot prompting on dozens of downstream tasks.\n\nRETRO - DeepMind showed retrieval-augmented generation using a 7-billion-parameter model trained with 2-trillion-token memory, pointing to memory efficiency for future giant models.\n\nJurassic-1 - AI21 Labs debuted a 178-billion-parameter transformer rivaling GPT-3, proving that an independent team could field comparable generative models in production.\n\n2021 - Human Alignment Gets Systematized\n\nCodex - OpenAI fine-tuned GPT on GitHub code, creating the engine beneath GitHub Copilot and pioneering transformer-based program synthesis.\n\nInstructGPT - OpenAI used “RL from Human Feedback” (RLHF) with PPO to fine-tune GPT-3 into helpful, harmless, and honest instructions followers, reducing toxic and untruthful outputs.\n\nLaMDA - Google unveiled Language Model for Dialogue Applications, showcasing extensive safety fine-tuning and conversational coherence at 137-billion parameters.\n\nMegatron-Turing NLG - Microsoft and NVIDIA trained a 530-billion parameter model, validating hardware-software co-design for efficient trillion-param training and explicit 3D parallelism.\n\n2022 - ChatGPT and the Conversational Revolution\n\nPaLM - Google Research trained Pathways Language Model (540 B parameters) with the Pathways system, showing still larger jumps in multilinguality, reasoning and code generation.\n\nChatGPT (public) - OpenAI released ChatGPT, a GPT-3.5 model fine-tuned via RLHF for dialogue; within two months it reached 100 million users, igniting an industry-wide race.\n\nMidjourney + StableDiffusion - While visual-generation models captured attention, their techniques (latent diffusion, human-feedback tuning) transferred ideas back to text alignment and safety tools for LLMs.\n\n2023 - Multimodality, Constitutional AI and GPT-4\n\nGPT-4 - OpenAI launched an opaquely trained multimodal transformer (text + vision) demonstrating expert-level performance on professional exams via scale, longer context, and further RLHF plus rule-based reward models (RBRMs).\n\nClaude - Anthropic released Claude 2 trained with “Constitutional AI”, a self-critique and preference-modeling procedure that reduces harmful outputs without as much human feedback compared to RLHF alone.\n\nGemini - Google DeepMind debuted Gemini Ultra, claimed to surpass GPT-4 on MMLU through mixture-of-experts and joint multimodal training.\n\nLlama-2 - Meta published an openly available 70 B parameter LLM optimized with safety fine-tuning, catalyzing an open-source LLM wave.\n\nDIRECT RL with LLMs - Anthropic demonstrated scaling reward-model-based RLHF to Claude, showing alignment at frontier scales remains tractable with PPO variants.\n\n2024 - System 2 Reasoning and Long-Context Scaling\n\nOpenAI o1 (Strawberry) - OpenAI introduced a test-time reasoning model that inserts internal chain-of-thought (CoT) Monte-Carlo rollouts during inference, trading compute for better math, coding, and science performance; it uses self-generated synthetic CoT data and reward-model PPO at inference-time.\n\nSonnet 3.5 - Anthropic released Claude 3.5 Sonnet, tripling the coding benchmark performance of Claude 3 via improved Constitutional RLHF and aggressive synthetic data.\n\nGemini 1.5 Pro - Google extended contexts to 2-million-token instruction-following and introduced “needle-in-a-haystack” retrieval techniques using mixture-of-experts and sparse attention.\n\nMamba-2 - The academic team behind Mamba released a state-space-model variant that rivals dense attention at 3–8 billion params, hinting at alternatives to Transformers for future scale.\n\nMeta CodeLlama-70B-Instruct and Mistral-8x22B showed the open-source frontier remains competitive at large scale while adopting RLHF safety practices more aggressively.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1387,
        "completionTokensWithoutThoughts": 1387,
        "totalTokens": 1586,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 35862,
      "id": "8debf239-fcf1-41e2-832a-b122527e853d",
      "createdAt": "2025-07-18T14:47:23.982Z",
      "notes": "Covers most points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "parasail"
        ]
      },
      "response": "2015 - Deep Learning Horizons\nDQN - Google DeepMind introduced the Deep Q-Network (DQN) that combined deep neural networks with Q-learning to master 49 Atari 2600 games directly from pixel inputs, showing the first practical large-scale success of deep reinforcement learning.  \nAlphaGo beats Fan Hui - Google DeepMind’s AlphaGo defeated European champion Fan Hui 5-0, proving neural networks + Monte-Carlo tree search can crack Go.  \nDistillation via “policy distillation” - DeepMind also proposed a technique to compress large RL policies into smaller networks, foreshadowing methods to scale down massive future language models.\n\n2016 - Superhuman at Go\nAlphaGo beats Lee Sedol - In Seoul, AlphaGo defeated 18-time world champion Lee Sedol 4-1, a historic milestone watched live by 200 million people and signalling RL’s arrival at super-human prowess.  \nA3C - Google DeepMind released Asynchronous Advantage Actor-Critic (A3C), showing multi-core CPU training could stabilize policy gradients and rival GPU approaches.  \nOpenAI Gym & Universe - OpenAI released Gym, the now-standard RL testbed, and Universe to benchmark agents across thousands of browser-based tasks.\n\n2017 - Attention is All You Need\nAttention Is All You Need paper - Google Brain & University of Toronto introduced the Transformer architecture, replacing recurrence with self-attention and enabling parallel training and unprecedented scaling for language tasks.  \nAlphaZero - Google DeepMind revealed AlphaZero, which learns tabula rasa by pure self-play in Go, chess and shogi, achieving superhuman mastery in 24 hours with the same algorithm.  \nProximal Policy Optimization (PPO) - OpenAI published PPO, taming policy-gradient variance via clipped surrogate objectives and becoming the RL algorithm of choice for language-model fine-tuning years later.\n\n2018 - Bidirectional Language Models\nBERT - Google released BERT (Bidirectional Encoder Representations from Transformers), demonstrating that unsupervised pre-training + fine-tuning equips NLP models with wide linguistic understanding, inspiring later GPT scaling.  \nGPT-1 - OpenAI introduced the 117 M-parameter Generative Pre-trained Transformer, showing that unsupervised language modeling followed by supervised fine-tuning yields strong downstream task performance.  \nEvolution Strategies comeback - OpenAI showed linear-scaling ES could match deep RL on Atari and MuJoCo using sparse rewards, expanding the toolbox for LLM reward modeling.\n\n2019 - Scaling Law Emergence\nGPT-2 1.5 B - OpenAI released GPT-2, whose high-quality long-form text prompted caution over release due to misuse potential and sparked mainstream debate on generative model governance.  \nT5 “Text-to-Text Transfer Transformer” - Google unified NLP tasks as text-to-text and showed encoder-decoder Transformers benefit from massive pre-training on C4.  \nMuZero - DeepMind combined AlphaZero planning with learned dynamics models, demonstrating model-based RL in pixel environments without human rules.\n\n2020 - The 175-B Leap\nGPT-3 175 B - OpenAI launched GPT-3, demonstrating emergent few-shot learning at scale and popularizing “prompting” as the new user interface for AI.  \nCLIP & DALL-E - OpenAI paired vision-language pre-training (CLIP) and generative image synthesis (DALL-E), showcasing Transformer foundations beyond text.  \nGLaM 1.2 T sparse - Google piloted sparse mixture-of-experts to match GPT-3 quality with 1/3 activation cost, exploring efficiency tools critical for later LLMs.\n\n2021 - Instruction & Alignment\nInstructGPT - Using the first large-scale deployment of Reinforcement Learning from Human Feedback (RLHF), OpenAI fine-tuned GPT-3 to follow instructions and reduce harmful outputs, a key alignment technique for today’s chatbots.  \nGopher 280 B - DeepMind released Gopher and accompanying paper on scaling laws, giving empirical grounding for compute-optimal training.  \nLaMDA - Google introduced LaMDA, an 137 B conversational model trained on dialogue and evaluated for safety and factual grounding.\n\n2022 - RLHF Goes Mainstream\nChatGPT - OpenAI launched the RLHF-fine-tuned sibling of GPT-3.5, reaching 100 million users in two months and making conversational AI ubiquitous.  \nPaLM 540 B - Google Research unveiled PaLM, achieving breakthrough English and multilingual performance via Pathways system for 2-d tensor parallelism.  \nAlphaTensor - DeepMind’s Transformer-ized RL discovered novel matrix-multiplication algorithms, illustrating general scientific-discovery potential.\n\n2023 - GPT-4 & Claude Race\nGPT-4 - OpenAI released GPT-4, a multimodal, RLHF-aligned model scoring in 90th percentile on the Uniform Bar Exam and supporting image and text inputs.  \nClaude 2 - Anthropic debuted Claude 2, trained with Constitutional AI and RLHF, emphasizing safety and very large context windows (100k tokens).  \nGemma / GeminiFamily - Google introduced Gemini Ultra and the open-source Gemma models, showing competitive benchmarks while emphasizing stewardship reports.\n\n2024 - Multimodal & Reasoning Boundaries\nClaude 3 (Opus/Sonnet/Haiku) - Anthropic achieved state-of-the-art on graduate-level reasoning and coding evaluations with near-instant tool-use and vision, cementing closed-source frontier labs’ lead.  \nGPT-4-turbo-2024-04 - OpenAI released a refined GPT-4 with improved instruction following and 128 k context, integrated into ChatGPT Team and Copilot.  \nAlphaGeometry - DeepMind’s neuro-symbolic system, trained via synthetic data and RL, solved 25 of 30 IMO geometry problems, hinting at mathematical reasoning inside future LLM variants.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1188,
        "completionTokensWithoutThoughts": 1188,
        "totalTokens": 1387,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 99794,
      "id": "8a2a92aa-4c8c-4b73-9e6a-f5a43a16b4e0",
      "createdAt": "2025-07-18T14:48:27.914Z",
      "notes": "Covers most points\nWrong format",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "together"
        ]
      },
      "response": "2015 - Deep networks master Atari\nHuman-level control through deep reinforcement learning, Mnih et al., DeepMind - The DQN algorithm shows a single neural network can master dozens of Atari games straight from pixels, solidifying the marriage of deep learning and RL.\nGoogle open-sources TensorFlow - Making large-scale neural network training practical for industry and research.\n\n2016 - AlphaGo conquers Go\nAlphaGo - Google DeepMind created AlphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCTS) with deep neural networks trained by supervised and reinforcement learning.\nAlphaGo-Lee Sedol match - The widely televised 4-1 victory in Seoul becomes global proof that deep RL can surpass millennia of human mastery.\n\n2017 - Attention and Transformers emerge\nAttention Is All You Need, Vaswani et al., Google Brain - Introduces the Transformer self-attention architecture, replacing recurrence with massive parallelization and laying the groundwork for future language models.\nAlphaGo Zero - DeepMind shows an RL system can exceed the superhuman version without any human game data, learning tabula rasa via self-play.\nProximal Policy Optimization (PPO), Schulman et al., OpenAI - A robust, easy-to-tune policy-gradient RL algorithm that quickly becomes the default choice for continuous control.\n\n2018 - Language models scale with self-supervised learning\nBERT, Devlin et al., Google - Bidirectional Encoder Representations from Transformers use masked-language modeling to set new records across NLP tasks, showing unsupervised pre-training plus fine-tuning outperforms training from scratch.\nOpenAI Five defeats human Dota 2 semi-professionals - A scaled-up PPO system with thousands of GPUs illustrates deep RL’s ability to handle complex team strategy games.\n\n2019 - Billion-parameter models and OpenAI’s Magnet\nGPT-2, Radford et al., OpenAI - A 1.5 B-parameter autoregressive Transformer trained on web text displays surprisingly coherent text generation; OpenAI initially withholds the full model, sparking debate on safety and responsible release.\nDeepMind release AlphaStar League champion - Self-play plus imitation and population-based training produces a StarCraft II agent ranked above 99.8 % of human players, highlighting multi-agent RL in real-time strategy.\n\n2020 - Scaling laws and the first 175 B-parameter LM\nScaling Laws for Neural Language Models, Kaplan et al., OpenAI - Empirically shows that cross-entropy loss decreases smoothly with more compute, model parameters, and data, giving the field a compute-driven roadmap.\nGPT-3, Brown et al., OpenAI - A 175 B-parameter Transformer achieves strong few-shot and zero-shot performance on hundreds of benchmarks, popularizing “prompting” and demonstrating meta-learning at web scale.\n\n2021 - Aligning and steering language models\nInstructGPT, Ouyang et al., OpenAI - Demonstrates Reinforcement Learning from Human Feedback (RLHF): fine-tuning GPT-3 with human preference labels plus PPO makes language models safer, more helpful, and easier to guide.\nGopher, Rae et al., DeepMind - Publishes a 280 B-parameter model and the accompanying 152-page technical report, advancing safety and toxicity tuning regimes.\nClaude v1 private beta, Anthropic - An RLHF-aligned conversational model focused on Constitutional AI and harmlessness research.\n\n2022 - ChatGPT catalyzes public adoption\nChatGPT launch, OpenAI - Fine-tuned from GPT-3.5 using RLHF and conversation data, it becomes the fastest-growing consumer application in history, turning “talking to AI” into a mainstream activity.\nFlamingo, Alayrac et al., DeepMind - Shows unified vision-language Transformer capable of few-shot multimodal tasks, foreshadowing chatbots that see and reason over images.\n\n2023 - The Claude and GPT-4 wave\nGPT-4, OpenAI - A multimodal Transformer trained on vision and text, with enhanced RLHF alignment and steerability, setting new performance ceilings on standardized tests and coding benchmarks.\nClaude 2 release, Anthropic - A constitutional reinforcement-learning–aligned model supporting 100 K-token contexts, pushing responsible scaling and effective-use paradigms.\nOpenAI, Google, Anthropic publish alignment and safety roadmaps - Public recognition that large-scale RLHF, red-teaming, and interpretability tools are now critical infrastructure for frontier models.\n\n2024 - Toward agentic AI and open long-context systems\nClaude 3 family (Haiku, Sonnet, Opus), Anthropic - Hybrid training stacks of unsupervised pre-training, RLHF, and Constitutional AI yield models competitive with GPT-4-Turbo on most benchmarks and pioneer long-context recall out to 200 K tokens.\nGPT-4-Turbo and GPT-4o releases, OpenAI - Demonstrates live voice and vision integration, showing the start of real-time multi-modal agents.\nOpen-source frontier models (Llama 3, Mistral Large, etc.) proliferate - Combined with sophisticated post-training recipes (DPO, PPO, and distilled variants), they democratize the techniques that once required the full RLHF stack, setting the stage for 2025’s next leap.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1075,
        "completionTokensWithoutThoughts": 1075,
        "totalTokens": 1274,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 25669,
      "id": "2df977f5-c585-4341-b37e-e1614588390c",
      "createdAt": "2025-07-18T14:51:44.693Z",
      "notes": "Covers major points\nWrong format",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "deepinfra"
        ]
      },
      "response": "2015 - The deep reinforcement learning awakening  \nDeep Q-Network (DQN) - DeepMind, led by Volodymyr Mnih, published the seminal Nature paper that used a convolutional neural network plus replay buffers and experience replay to surpass human performance on 49 Atari games, establishing deep RL as a viable path toward general intelligence.  \nPolicy-gradient explosion – researchers including David Silver at DeepMind and Sergey Levine (then at Google/UC Berkeley) published a flurry of scalable policy-gradient methods such as DDPG and TRPO, combining actor-critic models with fast GPU training and setting the stage for continuous-control RL breakthroughs.  \nAI safety emerges - OpenAI, founded by Sam Altman, Greg Brockman and Elon Musk among others, opens with a charter to ensure beneficial AGI, quickly hiring researchers like John Schulman to focus on reliable deep RL techniques that can be aligned to human intent.  \n\n2016 - From pixels to world champions  \nAlphaGo - Google DeepMind created AlphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCTS) with deep neural networks trained by supervised and reinforcement learning; it beat European Fan Hui 5-0 in October 2015 and Lee Sedol 4-1 in March 2016, amazing the world and crystallising excitement around deep RL.  \nOpenAI Gym - OpenAI released Gym, an open-source toolkit with benchmark suite that unified RL environments (Atari, MuJoCo, robotics) and enabled reproducible research across thousands of labs.  \nUnsupervised pretrained CNN progress - AlexNet-type architectures became the backbone for reinforcement and self-supervised learning (e.g., curiosity-driven exploration), turbocharging downstream RL vision tasks.  \n\n2017 - New architectures and the first attention language models  \nAttention Is All You Need - Vaswani et al. at Google Research introduced the Transformer architecture and multi-head self-attention, replacing recurrent models and enabling orders-of-magnitude faster training with better long-range dependencies, a prerequisite for the language-only training of future GPT models.  \nAlphaGo Zero & AlphaZero - DeepMind unveiled versions that removed human-knowledge priors and solely used self-play RL within a unified MCTS/search dispatcher, proving that superhuman performance can emerge from tabula rasa in both Go and Chess.  \nProximal Policy Optimisation (PPO) - John Schulman et al. at OpenAI published PPO, a clipped surrogate-objective on-policy RL algorithm that became the default backbone for continuous-control, launching hundreds of robotics and benchmark papers.  \n\n2018 - Cross-modal breakthroughs and scaling language understanding  \nBERT - Jacob Devlin and Google Research released Bidirectional Encoder Representations from Transformers, a Transformer encoder trained on masked-language-modelling using massive web text; it set new SOTA on GLUE, blazing the trail for large-scale self-supervised pre-training.  \nLarge-scale Automatic Curriculum - DeepMind introduced IMPALA, an asynchronous actor-learner distributed RL framework able to use thousands of CPU/GPU actors to train 100M+ frame environments, signalling the advent of industrial-scale RL.  \nCocktail-party release of GPT-1 - Alec Radford, Tom Brown and the OpenAI team demonstrated that a 117 B-PT transformer language model pretrained on WebText could be fine-tuned for downstream tasks, laying the first stone in a series that would soon explode in size and capability.  \n\n2019 - Roaring scaling laws and the birth of unsupervised policy transfer  \nGPT-2 - OpenAI shocked the world with GPT-2-1.5B, trained on 40 GB of Internet text; due to concern over “too coherent” text generation, the full model was initially withheld, igniting public debate about responsible disclosure and demonstrating emergent few-shot capabilities and in-context learning.  \nMuZero - DeepMind’s Schrittwieser et al. published MuZero, a model that learns a compact latent dynamics model and achieves superhuman play in Atari, Go, chess and shogi without ever seeing the true environment dynamics—first evidence that world-models can replace hand-crafted simulators, aligning with sample-efficient RL dreams.  \nHierarchical Vision-Language RL - Facebook AI and Google explored combining self-supervised language supervision (like BERT and COCO) with vision RL tasks, suggesting that future agents could learn goals expressed in natural language and democratising expert-free policy transfer.  \n\n2020 - Scaling toward trillion-parameter minds  \nGPT-3 - OpenAI released GPT-3-175B, showing near-linear scaling of knowledge, reasoning and creativity with model size–compute; public beta showcased few-shot “in-context learning”, spurring the prompt-engineering era and convincing investors and founders alike that next-token prediction was a viable path toward AGI.  \nCLIP & DALL·E - Radford et al. (OpenAI) and Ramesh et al. introduced CLIP (image-text joint embedding) and DALL-E (GPT-like decoder for text-to-image), demonstrating how multimodal pre-training can fuse vision and language without labels, a concept later internalised by controllable RL/vision-language agents.  \nAchilles’ Heel – alignment & RLHF - Paul Christiano et al. published the first aligned RLHF (Reinforcement Learning from Human Feedback) experiments on text summarisation, showing that human preference modelling used as an RL reward signal can make large models safer and more helpful—later becoming the meta-recipe to align later GPT iterations.  \n\n2021 - From playtime to production  \nInstructGPT - OpenAI deployed RLHF alignment techniques on GPT-3.5 (1.3B→6B variants), creating InstructGPT, the first large-scale language system explicitly optimised to follow instructions and reject harmful queries, shifting the field from “predict tokens” to “obey-safe-serve” objectives.  \nGato - DeepMind’s team (Reed, Cabi, Hutter) released Gato, a single 1.2B transformer that plays Atari, captions images, controls robot arms and chats with one set of weights, illustrating the feasibility of a generalist agent across modalities, foreshadowing LLMs that can learn new skills via additional embeddings instead catastrophic forgetting.  \nMegatron-LM & PaLM - NVIDIA (Shoeybi et al.) and Google (Chowdhery et al.) independently shipped 530B and 540B dense transformer language models, demonstrating memory-efficient tensor parallelism and robust jumps in benchmark scores such as MATH and BIG-Bench, fueling the arms race.  \n\n2022 - RL-trained conversation becomes the product  \nChatGPT - OpenAI combined GPT-3.5 with deeper RLHF, plus dialogue data and direct policy-gradient from PPO, releasing ChatGPT (November 2022), which gained 100 million users in two months and normalised conversant language agents as everyday software, transforming demos into real-world consumer products.  \nMinerva - Google Research unveiled Minerva-540B, explicitly fine-tuned for mathematical reasoning via chain-of-thought prompting, foreshadowing how few-shot LLMs could integrate step-by-step reasoning without external symbolic solvers.  \nStable Diffusion & the open-source surge - Stability AI released Stable Diffusion, handing the public 1.4B open-source latent diffusion weights; the same engineering (REINFORCE, PPO for reward finetuning) was repurposed by independent teams to steer any diffusion model with RLHF aesthetics, democratising controllable AI generation.  \n\n2023 - Scaling to Claude and GPT-4, plus tool-augmented RL agents  \nGPT-4 - OpenAI launched GPT-4-multimodal (likely >1T parameters/en-mixture), showcasing SOTA on exams such as the bar, SAT and MMLU while also processing text and images; it used more advanced RLHF and red-teaming to reduce jailbreaks and hallucinations than GPT-3.5 and was integrated into Bing Chat, Copilot and ChatGPT Plus.  \nClaude - Anthropic (formed by former OpenAI siblings Dario and Daniela Amodei) released Claude-2 and Claude-2.1, trained with Constitutional AI: a hybrid of self-critique and PPO fine-tuning where the model generates constitutional responses and critiques itself, achieving competitive performance while emphasising transparency and safety-research openness.  \nTool-use & Retrieval - OpenAI and Microsoft introduced plugins/retrieval-augmented generation for ChatGPT, using RL-driven function calling to query the web, WolframAlpha or code interpreters—turning language models into embodied RL agents operating in the digital world, letting users run code and execute multi-step tasks.  \n\n2024 - Frontier agent pipelines, long-horizon reasoning and legality  \no1 reasoning models - OpenAI previewed “o1” (internal codename “Strawberry”), which adds large-scale reinforcement learning search inside the transformer autoregressive loop, producing explicit internal chain-of-thought distilled through online RL, surpassing human performance on IMO and CodeForces contests for the first time.  \nClaude 3.5 Sonnet & computer-use - Anthropic released Claude 3.5 Sonnet with Computer-Use that leverages fine-tuned vision + RL to click and type through any GUI (web, spreadsheet, programming IDEs) a la human workflow, marking a shift toward long-horizon autonomous planning at consumer scale.  \nState-of-the-art vertical models - Google DeepMind Gemini 1.5-Pro with MoE up to 2M context, Meta Llama 3.1 (405B) open-source and Mistral Large 2 shook the market; all integrate advanced post-training RLHF pipelines (RFT, DPO, GRPO) for domain maths/coding/biology agents, cementing 2024 as the first full year where frontier LLMs co-evolve with high-bandwidth toolbots achieving production-grade autonomy.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 2043,
        "completionTokensWithoutThoughts": 2043,
        "totalTokens": 2242,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 34087,
      "id": "499eea84-0aa6-45d3-8113-e93f1b069bcc",
      "createdAt": "2025-07-18T14:51:53.110Z",
      "notes": "Covers most points\nwrong format",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "parasail"
        ]
      },
      "response": "2015 - Deep RL Breakthrough Year\nDQN in Nature - Google DeepMind published \"Human-level control through deep reinforcement learning\" in Nature, showing that a single neural network (DQN) could master dozens of Atari games end-to-end from pixels, cementing the marriage of deep nets and RL.\nAlphaGo Lee announced - DeepMind revealed that its Go program had beaten European champion Fan Hui 5-0, the first time a system defeated a top human on a full-sized board using a mix of supervised learning from human games and self-play RL.\nUniverse platform - Concurrent with the Atari work, OpenAI launched Gym (April) and Universe (December) to provide standardized RL environments and attract academic and hobbyist researchers to the field.\n\n2016 - Machines Beat Human Experts\nAlphaGo vs Lee Sedol - DeepMind’s tuned system stunned the world by defeating 18-time world champion Lee Sedol 4-1 in Seoul; the “move 37” creativity and end-game resignation became cultural moments that showcased deep RL’s power.\nA3C paper - DeepMind introduced Asynchronous Advantage Actor-Critic, a scalable, lightweight algorithm that allowed multiple CPU threads to train a single policy in parallel without experience replay, speeding research.\nOpenAI Gym public launch - OpenAI upgraded Gym with leaderboards and baselines, turning the toolkit into the de-facto shared language for RL prototyping and reproducible results.\n\n2017 - Attention Is All You Need\nAttention paper - Google Brain’s “Attention Is All You Need” by Vaswani et al. introduced the Transformer, a new neural architecture that replaced recurrence with self-attention, dramatically speeding training and setting the stage for language models to scale.\nAlphaZero - DeepMind replaced handcrafted knowledge entirely with pure self-play RL; starting from random weights, AlphaZero surpassed AlphaGo Lee after just three days of training and went on to master chess and shogi.\nCuriosity-driven RL - OpenAI and UC Berkeley showed agents that learn intrinsic curiosity signals could explore sparse-reward 3-D mazes without extrinsic supervision, foreshadowing unsupervised pre-training trends.\n\n2018 - Pre-trained Representations Arrive\nBERT - Google open-sourced Bidirectional Encoder Representations from Transformers, demonstrating that large-scale unsupervised pre-training plus fine-tuning could beat task-specific architectures on eleven NLP benchmarks.\nIMPALA - DeepMind released a scalable, batched actor-learner architecture able to soak up thousands of CPU environments, pushing single-agent performance in DMLab-30 and Atari.\nOpenAI Five - Using PPO on 256 GPUs and 128,000 CPU cores, OpenAI’s Dota 2 team beat semi-pro humans, hinting at the computational appetite of future frontier models.\n\n2019 - Extensive Self-Play & Language Control\nOpenAI Five Finals - The refined five-hero roster defeated the world champions OG 2-0 in a live San-Francisco match and later allowed public players to lose repeatedly, proving multi-agent RL could reach super-human teamwork.\nGPT-2 release - OpenAI published “Language Models are Unsupervised Multitask Learners,” releasing first 117 M and later 1.5 B parameter models; the staged rollout due to “too-real fake news” sparked global discussion of AI safety and misuse.\nMuZero - DeepMind unveiled MuZero, matching AlphaZero’s performance while learning a latent model of environment dynamics without ever seeing the rules, merging planning and learning in one loop.\n\n2020 - Scaling Hits GPT-3\nGPT-3 paper - OpenAI published “Language Models are Few-Shot Learners,” showing a 175 B parameter Transformer that could perform downstream tasks with in-context prompts alone, igniting the race for ever-larger language models.\nDALL·E v1 - Built on the GPT-3 backbone, OpenAI demonstrated a 12 B parameter diffusion-based model that generated images from text captions, extending generative pre-training beyond language.\nAlphaFold 2 - While not traditional RL, DeepMind’s attention-based model solved 50-year-old protein-folding problem, illustrating the cross-domain power of large-scale deep learning and re-using self-attention machinery.\n\n2021 - Learning from Preferences\nInstructGPT - OpenAI showed that a 1.3 B model fine-tuned with human feedback (RLHF) could outperform the raw 175 B GPT-3 on user intent alignment, establishing RLHF as the secret sauce behind helpful, safe assistants.\nCodex - A GPT derivative fine-tuned on GitHub solved 72 % of Python coding tasks and powered GitHub Copilot, proving large LMs could function as interactive pair programmers.\nGopher & RETRO - DeepMind launched a 280 B language model and a retrieval-enhanced 7 B model focusing on factual accuracy, joining Google’s LaMDA and PaLM in the “land-rush” of giant models.\n\n2022 - RLHF Goes Mainstream\nChatGPT - OpenIA released a conversational fine-tune of GPT-3.5 trained with RLHF from human labelers, amassing 100 M users in two months and bringing aligned dialogue to the mass market.\nChinchilla paper - DeepMind’s “Training Compute-Optimal Large Language Models” overturned the “bigger is always better” mantra by showing that equal investment in data and parameters yields smaller, cheaper, better models like the 70 B Chinchilla.\nStable Diffusion - Stability AI open-sourced a latent-diffusion text-to-image model, proving that consumer-grade GPUs could run cutting-edge generative AI and energizing open-source communities.\n\n2023 - Enterprise AI Leap\nClaude 2 - Anthropic shipped a 100 K-token-context assistant trained with “Constitutional AI,” using self-critique and RL from AI feedback to scale human oversight.\nGPT-4 launch - OpenAI revealed a multimodal Transformer that passes the bar exam, accepts images, and powers ChatGPT Plus and Microsoft Copilot consoles; a six-month safety “red-team” preview introduced pre-release policy debates.\nRL at Scale Problem - DeepMind’s AlphaDev discovered faster sorting algorithms via deep RL acting directly on assembly instructions, hinting that RL can still squeeze gains when deployed inside foundational models.\n\n2024 - Multimodal Powerhouses\nGemini 1.5 Pro - Google DeepMind launched a 1 M token context model and native audio-video understanding, merging the Chinchilla scaling lessons with the first trillion-token training corpus.\nClaude 3 - Anthropic released Claude 3 (Haiku, Sonnet, Opus) beating GPT-4 on broad benchmarks while emphasizing safety; Frontier Model Forum and White House commitments revealed industry-wide self-regulation around foundation models.\nSora - OpenAI unveiled Sora, a diffusion-Transformer hybrid that generates photorealistic 60-second videos, showing the Transformer + RL feedback loop now extends across sight, sound and motion.",
      "reasoning": "",
      "usage": {
        "promptTokens": 199,
        "completionTokens": 1410,
        "completionTokensWithoutThoughts": 1410,
        "totalTokens": 1609,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 118755,
      "id": "d61c2a96-4e6f-478e-9096-7c972d1b2cc9",
      "createdAt": "2025-07-18T14:54:05.770Z",
      "notes": "Covers most points\nWrong format",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 9,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "moonshotai"
        ]
      },
      "response": "2015 - Deep Learning Breakthroughs and RL Fundamentals\n\nDQN - Google DeepMind developed Deep Q-Network (DQN), the first deep reinforcement learning agent to reach human-level performance on many Atari games, combining Q-learning with convolutional neural networks.\n\nBatch Normalization - Google researchers introduced Batch Normalization, a technique that dramatically improves training speed and stability of deep networks, enabling deeper architectures.\n\nOpenAI founded - Elon Musk, Sam Altman, and others created OpenAI as a non-profit AI research company dedicated to ensuring AGI benefits all of humanity.\n\n2016 - Reinforcement Learning Mastery\n\nAlphaGo - Google DeepMind created AlphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\nMastering the Game of Go with Deep Neural Networks and Tree Search - DeepMind published their seminal Nature paper detailing how AlphaGo combined deep learning and Monte Carlo tree search to achieve superhuman Go performance.\n\nA3C - DeepMind introduced Asynchronous Advantage Actor-Critic (A3C), a parallelized RL architecture that achieved state-of-the-art results while running on multiple CPU cores without shared memory.\n\n2017 - Attention Is All You Need\n\nAttention Is All You Need - Google researchers published the Transformer paper, introducing the attention mechanism that revolutionized neural language processing and became the foundation for all modern LLMs.\n\nAlphaGo Zero - DeepMind created AlphaGo Zero, which learned to play Go from scratch without human data, achieving superhuman performance through pure self-play and reinforcement learning.\n\nDec 2017 - OpenAI released Universe, a software platform for measuring and training AI across games, websites, and applications.\n\n2018 - Self-Supervised Learning Emerges\n\nGPT-1 - OpenAI released the first Generative Pre-trained Transformer (GPT), demonstrating that unsupervised pre-training on large text corpora followed by supervised fine-tuning could achieve strong language understanding.\n\nBERT - Google introduced Bidirectional Encoder Representations from Transformers (BERT), revolutionizing NLP tasks through masked language modeling and bidirectional context understanding.\n\nLarge-scale General Language Models - OpenAI published research on large-scale unsupervised language models, providing early evidence that scaling model size would lead to emergent capabilities.\n\n2019 - Scaling Hypothesis Validated\n\nGPT-2 - OpenAI released GPT-2 with 1.5 billion parameters, showing such significant capabilities that they initially withheld the full model due to concerns about misuse, validating the scaling hypothesis.\n\nT5 - Google introduced Text-to-Text Transfer Transformer (T5), unifying all NLP tasks into a text-to-text paradigm while exploring the boundaries of model scaling.\n\nAlphaStar - DeepMind achieved grandmaster level in StarCraft II using AlphaStar, demonstrating RL's ability to master complex real-time strategy games through distributed training and multi-agent learning.\n\n2020 - GPT-3 Emerges\n\nGPT-3 - OpenAI released GPT-3 with 175 billion parameters, demonstrating emergent abilities including few-shot learning, in-context learning, and creative writing abilities, surprising researchers with capabilities not present in smaller models.\n\nScaling Laws for Neural Language Models - OpenAI published comprehensive research showing how language model performance scales predictably with model size, data, and compute, providing a roadmap for future development.\n\nRL from Human Feedback (RLHF) techniques began to be developed, with early papers exploring how human preferences could guide language model behavior.\n\n2021 - Alignment and Fine-tuning\n\nInstructGPT - OpenAI released InstructGPT, demonstrating that fine-tuning large language models using RLHF could dramatically improve instruction-following capabilities and reduce harmful outputs.\n\nChinchilla - DeepMind published the Chinchilla paper, revising AI scaling laws to emphasize data efficiency, revealing that many models had been undertrained relative to optimal compute budgets.\n\nCodex - OpenAI released Codex, a GPT model fine-tuned on code that powers GitHub Copilot, showing LLMs could learn to program from examples.\n\n2022 - GPT-4 Development and LaMDA\n\nChatGPT - OpenAI launched ChatGPT in November, fine-tuned using RLHF from GPT-3.5, becoming the fastest-growing consumer application in history with 100M users in two months.\n\nLaMDA 2 - Google announced LaMDA 2, their advanced conversational AI, though remained more limited in public access compared to OpenAI's approach.\n\nConstitutional AI - Anthropic pioneered Constitutional AI, an alternative to RLHF that uses a set of principles to guide model behavior through self-critique and revision.\n\n2023 - Claude and the Rise of AI Assistants\n\nGPT-4 - OpenAI released GPT-4, a large multimodal model showing remarkable improvements in reasoning, reduced hallucinations, and the ability to handle both text and images.\n\nClaude - Anthropic launched Claude, built on Constitutional AI principles for safety and usefulness, focusing on helpful, harmless, and honest responses.\n\nLlama 2 - Meta released Llama 2, a family of open-source LLMs up to 70B parameters, making powerful language models accessible to researchers and developers worldwide.\n\n2024 - Agentic Systems and Advanced Reasoning\n\nClaude 3 - Anthropic released Claude 3 family (Haiku, Sonnet, Opus) in March, with Claude 3.5 Sonnet in June, each showing increasingly sophisticated reasoning, coding, and visual understanding capabilities.\n\nGemini 1.5 - Google released Gemini 1.5 Pro with an unprecedented 2 million token context window, enabling processing of entire books or large codebases in a single prompt.\n\no1-preview - OpenAI introduced o1-preview/o1-mini in September, featuring enhanced reasoning capabilities that spend more time \"thinking\" before responding, particularly effective at complex tasks in math, science, and coding.",
      "reasoning": "",
      "usage": {
        "promptTokens": 190,
        "completionTokens": 1185,
        "completionTokensWithoutThoughts": 1185,
        "totalTokens": 1375,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 117227,
      "id": "67c67350-ab11-452e-a60a-3d071d02e4e2",
      "createdAt": "2025-07-18T15:58:51.906Z",
      "notes": "Covers most points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 8.5,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "moonshotai"
        ]
      },
      "response": "2015 - The age of super-human deep reinforcement learning begins  \nDQN on Atari - DeepMind published \"Human-level Control through Deep Reinforcement Learning\", showing a single neural network (DQN) could master 49 Atari games from raw pixels, popularizing replay buffers and target networks in RL.  \nOpenAI founded - Sam Altman, Elon Musk, Ilya Sutskever and others launched OpenAI to ensure AGI benefits all of humanity, accelerating open-source RL and language-model research.  \nAlphaGo vs Fan Hui - Google DeepMind’s AlphaGo, using supervised learning on human games plus self-play RL and MCST, secretly defeated European Champion Fan Hui 5-0, the first time a computer beat a top pro at Go.\n\n2016 - Super-human Go, Boston Dynamics parkour, and OpenAI Gym  \nAlphaGo vs Lee Sedol - DeepMind’s AlphaGo defeated Lee Sedol 4-1 in Seoul, viewed live by 200 million people and initiating global AI awareness.  \nOpenAI Gym released - OpenAI’s Gym provided a shared, reproducible suite of RL environments (from classic control to Atari), becoming the lingua franca of RL benchmarking.  \nTrust Region Policy Optimization (TRPO) - John Schulman and colleagues at OpenAI introduced monotonic policy improvement via KL-constrained updates, enabling stable deep RL for continuous control.\n\n2017 - Transformer eruption  \nAttention is All You Need - Google Brain’s Ashish Vaswani et al. introduced the Transformer architecture, replacing recurrence with self-attention and giving rise to today’s LLMs.  \nAlphaZero - DeepMind generalized AlphaGo into AlphaZero, an RL system that learns solely by self-play Tabula-rasa, mastering Go, chess and shogi in a single network and inspiring efficient meta-learning.  \nProximal Policy Optimization (PPO) - OpenAI’s Schulman et al. released PPO, a robust, easy-to-tune policy-gradient algorithm that quickly became the default RL baseline.\n\n2018 - Pre-training language across the web  \nGPT-1 - OpenAI released “Improving Language Understanding by Generative Pre-Training”, a 117 M-parameter Transformer pre-trained on BookCorpus, showing unsupervised pre-training plus discriminative fine-tuning could outperform supervised baselines across many NLP tasks.  \nBERT - Google released BERT (Bidirectional Encoder Representations from Transformers), pre-trained with masked-language-model objectives and dramatically setting new records on GLUE, SQuAD and more.  \nIMPALA architecture - DeepMind’s Lasse Espeholt et al. launched IMPALA, a scalable, V-trace-corrected actor-learner system that efficiently trains agents across hundreds of environments.\n\n2019 - Scaling scripts controllable text and reasoning  \nGPT-2 - OpenAI unveiled GPT-2 (1.5 B parameters), demonstrating that simply scaling the same transformer and data pipeline yields coherent multi-paragraph text and prompting concerns about misuse, leading to staged disclosure.  \nAlphaStar in StarCraft II - DeepMind’s AlphaStar reached Grandmaster level on the full 1v1 ladder by using multi-agent RL, league training and imitation+self-play, conquering the final major e-sport benchmark.  \nMuZero - DeepMind introduced MuZero, which learns a latent dynamics model of the environment through pure planning, achieving Atari SOTA and super-human performance in board games without ever seeing the rules.\n\n2020 - GPUs meet the scaling law  \nGPT-3 - OpenAI released GPT-3 (175 B parameters), showing few-shot learning without gradient updates—translation, arithmetic, chat, code—confirming emergent abilities with scale and sparking the commercial LLM wave.  \nAlphaFold 2 - DeepMind solved the 50-year protein-folding problem by combining transformers with geometric RL-inspired refinement, delivering structures within 1 Å accuracy and accelerating biology research.  \nEfficientZero - Researchers from UC Berkeley and DeepMind introduced EfficientZero, distilling MuZero techniques into sample-efficient Atari agents matching human performance after only 60 000 frames (2 hours of play).\n\n2021 - Aligning and steering language models  \nInstructGPT - OpenAI combined RLHF (reinforcement learning from human feedback) with PPO to fine-tune GPT-3 into InstructGPT, demonstrating marked human preference gains and safer, more helpful behavior.  \nCodex & GitHub Copilot - OpenAI and Microsoft launched Codex (a GPT descendant fine-tuned on code) and subsequently Copilot, bringing LLM-powered pair-programming to millions of developers.  \nScaling Laws for Neural Language Models - DeepMind’s “Chinchilla” paper re-examined compute-optimal training, revealing that most large models were data-starved rather than parameter-heavy, reshaping resource allocations upwards.\n\n2022 - Alignment goes mainstream and diffusion paints  \nChatGPT - OpenAI released a conversational assistant tuned with RLHF from InstructGPT, reaching 100 M users in two months and making AI dialogue a daily consumer experience.  \nPaLM - Google trained the 540 B-parameter Pathways Language Model, showing strong performance across reasoning, multilingual and BIG-Bench tasks under Google’s new Pathways system.  \nStable Diffusion v1 - Stability AI together with LMU Munich open-sourced a latent diffusion text-to-image model, democratizing high-quality generative art and hinting at multimodal scaling paths for LLMs.\n\n2023 - Breakout agents, CLAUDE, Gemini and GPT-4 turbo-tokens  \nGPT-4 - OpenAI released GPT-4, adding multi-modal vision and dramatically stronger reasoning, now powering Bing, ChatGPT Plus and numerous apps.  \nGemini 1.0 - Google unveiled Gemini Ultra, proclaiming SOTA across MMLU, math and coding benchmarks and demonstrating native multimodal pre-training from the outset.  \nClaude 2 - Anthropic launched Claude 2, emphasizing Constitutional AI harmlessness and 100 k-token context windows as direct competitors to GPT-4.\n\n2024 - Large reasoning, open weights and real-time agents  \nClaude 3 - Anthropic’s Claude 3 family (Haiku, Sonnet, Opus) pushed state-of-the-art STEM reasoning and introduced near-instant “computer use” tool-calling for autonomous agent loops.  \nGPT-4o - OpenAI released “o” (omni), a natively audio-vision-text-optimized Transformer that reasons in real time with sub-second latency and multimodal voice/video.  \nSource-available frontier weight releases - Meta’s Llama 3 (8 & 70 B) and Mistral-Large-2 open-weight checkpoints rival closed models on benchmarks, igniting an ecosystem of private, fine-tuned LLMs and dense-and-MoE hybrids.",
      "reasoning": "",
      "usage": {
        "promptTokens": 190,
        "completionTokens": 1388,
        "completionTokensWithoutThoughts": 1388,
        "totalTokens": 1578,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 140120,
      "id": "ef28367c-8494-448e-8083-794b93b41f02",
      "createdAt": "2025-07-18T16:02:50.914Z",
      "notes": "Covers most points\nWrong format",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "moonshotai/kimi-k2",
      "provider": "openrouter",
      "rating": 9,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "providerOptions": {
        "only": [
          "moonshotai"
        ]
      },
      "response": "2015 - Deep reinforcement learning makes its breakout\n\nDQN - DeepMind published \"Human-level control through deep reinforcement learning\", introducing the first deep Q-network that could play many Atari2600 games directly from pixels, fusing convolutional vision models with Q-learning and experience replay.\n\nKtX - Keras and TensorFlow 1.0 stabilized user-friendly deep-learning workflows, giving researchers the tools needed to run large-scale RL experiments without custom CUDA kernels.\n\nAutonomous robot grasping - Google’s Robotics division trained a deep network to teach an off-the-shelf robot arm to grasp novel objects with 96 % success by using 800 thousand trial-and-error grasp attempts, showing that large data beats careful tuning in real-world control.\n\nHinton distillation - Geoff Hinton introduced “distillation”, a technique later critical for distilling giant language models into smaller versions that can still reason, laying groundwork for GPT parameter compression.\n\n2016 - Games and language models converge\n\nAlphaGo - Google DeepMind created AlphaGo, the first program to defeat a world champion at Go by combining Monte Carlo tree search with deep value and policy networks pre-trained on 30 million human moves and fine-tuned by self-play reinforcement learning, ending the 2,500-year human dominance of the game.\n\nA3C - DeepMind’s “Asynchronous Advantage Actor-Critic” removed the need for experience replay by running parallel agents on CPU threads, making on-policy RL training possible on single machines.\n\nWaveNet - DeepMind learned to synthesize raw speech waveforms with dilated convolutions, bringing autoregressive modeling into speech; the same causal Transformer-like structure would be adapted to language.\n\nOpenAI Gym/Universe - OpenAI released Gym and Universe, open benchmarks (including Atari, MuJoCo and browser-based tasks) that normalized RL research reproducibility and later seeded the data needed for RLHF.\n\n2017 - Attention displaces recurrence\n\nTransformer - Google Brain team’s “Attention Is All You Need” introduced the Transformer architecture, a 65-million-parameter language model trained in 3.5 days that replaced recurrence with self-attention, setting the template for every subsequent LLM.\n\nAlphaGo Zero - DeepMind removed hand-crafted features and human data by training a single neural net solely through self-play RL, starting from random weights and surpassing AlphaGo within 40 days, demonstrating pure reinforcement learning’s power to scale.\n\nPPO - OpenAI released Proximal Policy Optimization, a stable policy-gradient algorithm that reconciles actor–critic on-policy learning with trust-region clipping, becoming the default RL workhorse in robotics, RLHF and games.\n\nStable Baselines - A community rewrite of OpenAI’s Baselines made standardized implementations of DQN, PPO, TRPO accessible, accelerating reproducible work.\n\n2018 - Large pre-trained language models appear\n\nGPT-1 - OpenAI published “Improving Language Understanding by Generative Pre-Training”, scaling Transformers to 117 M parameters on BookCorpus and demonstrating that unsupervised pre-training plus supervised fine-tuning yields strong NLP transfer.\n\nBERT - Google released the Bidirectional Encoder Representations from Transformers (340 M parameters), fine-tuned across 11 NLP benchmarks to achieve state-of-the-art with ingenious MLM+NSP objectives, igniting the race for masked-language model pre-training.\n\nIMPALA - DeepMind scaled actor–learner architectures to 500 concurrent workers with V-trace importance sampling, showing how near-linear scaling makes RL faster rather than stale.\n\nSim-to-Real quadruped - ANYbotics and ETH Zurich trained ANYmal in simulation using PPO plus domain randomization and deployed it on rough terrain, demonstrating that RL policies simulated at scale can zero-shot transfer to hardware.\n\n2019 - Self-play meets language; scaling laws arrive\n\nGPT-2 - OpenAI scaled Transformers to 1.5 B parameters (“too dangerous to release”), showing emergent paragraph-level coherence and sparking the first mainstream debate on generative misinformation.\n\nOpenAI Five - OpenAI trained 170 000 simultaneous Dota 2 agents using PPO with long-horizon reward shaping, defeating the world champions OG in a two-match sweep and revealing how massive self-play plus RL can master 20k-agent-minute games.\n\nMuZero - DeepMind closed the loop between model-based and model-free RL by learning to plan with a learned latent dynamics model, achieving human-level performance at Go, chess, shogi and 57 Atari games without knowing the rules.\n\nXLNet - CMU & Google proposed a permutation-based autoregressive pre-training that supersedes BERT across 20 tasks, showcasing continued architectural innovation before compute-only scaling dominated.\n\nGPT “Scaling Laws” paper - OpenAI showed that performance improves smoothly as a power law versus model size, data and compute, giving the industry numerical justification to chase parameter counts.\n\n2020 - The first GPT-3 moment and RLHF emerge\n\nGPT-3 - OpenAI released GPT-3, scaling to 175 B parameters and exhibiting few-shot and zero-shot learning without fine-tuning; it is the first widely licensed LLM integrated into products like the API that sparks a wave of startups.\n\nRetNet - DeepMind introduced Retrieval-Enhanced Reinforcement Learning to add 1 trillion tokens of external memory to language agents, bridging factual recall with policy search.\n\nSwiftine - UC Berkeley trained transformer-based policies to keep a humanoid Cassie robot upright entirely via RL in simulation and deployed on hardware, proving Transformer controllers transfer.\n\nRLHF - OpenAI’s “Concrete Problems” refinement plus the “Reinforcement Learning from Human Feedback” paper systematize collecting human comparisons to train reward models and fine-tune language agents for human-aligned dialogue.\n\nSimCLR v2 - Google contrastive self-supervised visual representations enabled RL perception of nuanced object interactions, indirectly lifting sample-efficiency for vision-and-language tasks.\n\n2021 - Code and multimodality as frontiers\n\nCodex - OpenAI distilled a GPT-3 variant on 159 GB of GitHub code to create Codex, which powers GitHub Copilot and writes 30 % of newly typed code in the wild.\n\nAlphaFold 2 - DeepMind Models folded proteins at atomic accuracy (CASP 14 median GDT_TS 92.4), showing attention architectures can solve hard real-world symbolic optimization problems and redefining what “task” RL can solve.\n\nPaLM Chinchilla - Google Brain’s 540 B PaLM and Gopher/DeepMind Chinchilla papers resolved compute scaling versus data ratios (the “Chinchilla optimal” rule) and produced top-ten superhuman coding, math and reasoning benchmarks.\n\nInstructGPT - OpenAI applied RLHF to 1.3 B parameters, outperforming 175 B vanilla GPT-3 on adherent responses, proving human feedback can yield smaller aligned models.\n\nMultimodal CLIP - OpenAI trained a text-image joint embeddings via 400 million image-caption pairs that became the supervisor reward model for later RL vision-language agents including DALL-E.\n\n2022 - ChatGPT brings RLHF to the public\n\nChinchilla Recalibration - DeepMind revised data-scaling laws (20× more tokens than parameters), guiding the next wave of compute-efficient training runs.\n\nSparrow - DeepMind published a 70 B fine-tuned LLM that uses RLHF and search-retrieval mitigations to reduce fabrication, foreshadowing responsible fine-tuning.\n\nChatGPT - OpenAI launched the web ChatGPT interface, a fine-tuned GPT-3.5 model further alignment-trained with RLHF, gaining 100 million users in two months and making aligned conversational AI a consumer utility.\n\nMake-a-Video - Meta released text-to-video generation via spatiotemporal transformers, showing modality expansion using large-scale models.\n\nImagen & Parti - Google showed class-guidance diffusion and dense-attention text-to-image synthesis at 4K resolution, directing future diffusion preference modeling.\n\n2023 - Open-source competition and the Claude / GPT-4 generation\n\nGPT-4 - OpenAI released GPT-4 (1.76 T MoE) with multimodal vision, scores >90 % on bar exam, and became the backbone for Microsoft Copilot and ChatGPT Plus.\n\nClaude 2 - Anthropic introduced Claude, a Constitutional AI agent pre-trained at 100k-token context, iteratively rewritten by self-critique and RL from human/AI feedback, positioning safety-through-scaling.\n\nLlama 2 - Meta open-sourced Llama-2-70B with commercial license, spawning 1000s of fine-tunes and heralding the era of widely accessible foundation RLHF research.\n\nDPO and RLAIF - Stanford introduced Direct Policy Optimization and Google proposed “Reinforcement Learning from AI Feedback”, showing reward models can be bootstrapped from reward-free LLMs themselves, drastically lowering human-labeling cost.\n\nAutoGPT & Agent Frameworks - Swyx’s open-source experiment glued LLM planners with external tools (Python, REST, search) to create autonomous agents, pointing to RL-in-the-loop agents.\n\nSORA tensors - Nvidia announced scaling programs reaching 25000-H100 clusters, enabling GPT-4-scale training in weeks instead of months, shifting focus to data center supply constraints.\n\n2024 - Frontier reasoning and robotics\n\nClaude 3 - Anthropic released Claude 3 (Haiku-Sonnet-Opus suite) with 200k context and SOTA undergraduate math tasks, demonstrating near-perfect retrieval and safety vetting through RL at scale.\n\nGemini 1.5 Pro - Google DeepMind shipped a natively multimodal 1.5-million-token sparse MoE model driven by U-PaLM-style online RL fine-tuning across text, audio, image and code.\n\nGPT-4o - OpenAI released Omni, a single end-to-end Transformer simultaneously handling speech, vision and text with near-real-time (<320 ms) latency, learned by collective RL across modalities.\n\no1 Reasoning - OpenAI unveiled Project “Strawberry” (o1) that accumulates internal chain-of-thought via self-consistency decoding and acts as its own teacher, achieving ~US Math Olympiad Top 1 % and signaling pure reasoning RL inside LLMs.\n\nO3 Omni Robotics - A DeepMind-Figure team combined Gemini perception with diffusion-policy RL to create a robotic table bussing agent that learned 100 dexterous tasks in one six-hour fine-tuning run,bridging gap between LLM planning and low-level control.\n\nEmu3 - Baidu open-released the 253 B MoE RLHF-trained bilingual ChatGPT-4-class model under a permissive Apache-style license, completing the transition of proprietary scale to community access.",
      "reasoning": "",
      "usage": {
        "promptTokens": 190,
        "completionTokens": 2178,
        "completionTokensWithoutThoughts": 2178,
        "totalTokens": 2368,
        "thoughtsTokens": 0
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 218975,
      "id": "cab9c4e6-20ee-4da2-b84a-de4f45086928",
      "createdAt": "2025-07-18T16:07:52.389Z",
      "notes": "Covers most points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "gemini-2.5-pro",
      "provider": "google",
      "rating": null,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "response": "2015 - Deep Learning Matures and Reinforcement Learning Gets a Boost\n\nDeep Q-Networks (DQN) - Google DeepMind published its groundbreaking paper on Deep Q-Networks in Nature. This work successfully combined deep neural networks with a classic reinforcement learning algorithm called Q-learning. The resulting agent was able to achieve superhuman performance on a wide range of Atari 2600 games by learning directly from pixel inputs, setting a new standard for general RL agents.\n\nResidual Networks (ResNet) - Researchers at Microsoft Research introduced the Residual Network. By using \"skip connections,\" ResNets allowed for the training of neural networks that were far deeper (hundreds or even thousands of layers) than previously possible without suffering from the vanishing gradient problem. This architectural innovation became fundamental to building the extremely large models that would follow.\n\n2016 - AI Achieves a Landmark Victory in a Complex Game\n\nAlphaGo - Google DeepMind's AlphaGo defeated Lee Sedol, one of the world's top Go players, in a 4-1 match. This was a monumental achievement, as Go is vastly more complex than chess. AlphaGo combined Monte Carlo tree search (MCTS) with deep neural networks trained through supervised learning (on human games) and reinforcement learning (by playing against itself), demonstrating the power of hybrid AI systems.\n\n2017 - A Foundational Architecture and Key Algorithms Emerge\n\nAttention Is All You Need (The Transformer) - Researchers at Google published a paper introducing the Transformer architecture. It replaced recurrent layers (RNNs/LSTMs) with a mechanism called self-attention, allowing the model to weigh the importance of different words in an input sequence. Crucially, this design enabled massive parallelization, which was the key to scaling models to unprecedented sizes. The Transformer is the foundational architecture for nearly all modern LLMs, including GPT and Claude.\n\nProximal Policy Optimization (PPO) - OpenAI released PPO, a new family of reinforcement learning algorithms. PPO improved upon existing policy gradient methods by being more stable, easier to implement, and more sample-efficient. It quickly became a default and highly effective RL algorithm used in a wide variety of tasks, including later for fine-tuning language models.\n\nAlphaGo Zero - In a follow-up to AlphaGo, DeepMind created AlphaGo Zero. This version was even more powerful because it learned to play Go entirely from scratch through self-play, without using any human game data. It demonstrated that pure reinforcement learning could surpass human knowledge in a complex domain.\n\n2018 - The Era of Large-Scale Pre-training Begins\n\nBERT (Bidirectional Encoder Representations from Transformers) - Google released BERT, a model that revolutionized the field of Natural Language Processing (NLP). It used the encoder part of the Transformer architecture and was pre-trained using a novel \"masked language model\" objective. This allowed it to learn deep bidirectional context (understanding a word based on all other words in a sentence, not just those that came before it), setting new state-of-the-art records on a suite of NLP benchmarks.\n\nGPT-1 (Generative Pre-trained Transformer) - OpenAI introduced the first Generative Pre-trained Transformer. It used the decoder part of the Transformer architecture and demonstrated the effectiveness of a two-stage process: unsupervised pre-training on a massive, unlabeled text corpus, followed by supervised fine-tuning for specific tasks. This established the \"pre-train and fine-tune\" paradigm that became dominant.\n\n2019 - Scaling Shows Surprising Emergent Abilities\n\nGPT-2 - OpenAI developed GPT-2, a much larger version of its predecessor. The model was so effective at generating coherent and contextually relevant text that OpenAI initially opted for a staged release due to concerns about potential misuse. GPT-2 demonstrated that by simply scaling up the model size and training data, new capabilities—like zero-shot task completion—could emerge without explicit training.\n\n2020 - Scaling Becomes a Science and a Spectacle\n\nGPT-3 - OpenAI released GPT-3, an even larger model with 175 billion parameters. Its ability to perform a wide range of tasks with only a few examples provided in a prompt (few-shot learning), without any fine-tuning, was a stunning demonstration of the power of scale. The release of its API allowed developers worldwide to start building applications on top of a large language model for the first time.\n\nScaling Laws for Neural Language Models - Researchers at OpenAI published work that formalized the relationship between model performance, model size, dataset size, and the amount of training compute. These \"scaling laws\" provided a predictable recipe for creating more capable models by showing that performance reliably improved as these factors were increased, justifying the massive investment required to train models like GPT-3.\n\n2021 - Generative AI Expands Beyond Text\n\nDALL-E - OpenAI showcased DALL-E, a 12-billion parameter version of GPT-3 trained to generate images from text descriptions. It demonstrated that the Transformer architecture could be applied to create novel and complex images, bridging the gap between language and vision and kicking off a wave of interest in multimodal AI.\n\nGitHub Copilot - GitHub, in partnership with OpenAI, launched GitHub Copilot. Powered by a specialized OpenAI model called Codex (a descendant of GPT-3), it functions as an \"AI pair programmer\" that suggests code and entire functions to developers in real-time. It was one of the first widely adopted, commercially successful products built on a large-scale generative model.\n\n2022 - AI Becomes Conversational and Aligned with Human Intent\n\nInstructGPT / RLHF - OpenAI published a paper detailing how to align language models with user intent using Reinforcement Learning from Human Feedback (RLHF). This technique involves using human raters to create a preference dataset, which is then used to train a reward model. This reward model is subsequently used to fine-tune the LLM with reinforcement learning (like PPO), making it more helpful, truthful, and harmless. This was the key technique that made ChatGPT possible.\n\nChatGPT - In November, OpenAI released ChatGPT, a conversational AI built on a model from the GPT-3.5 series and fine-tuned using RLHF. Its user-friendly chat interface made the power of LLMs accessible to the general public, leading to viral adoption. It became a cultural phenomenon, kicked off an industry-wide AI arms race, and demonstrated the immense value of aligning models for helpfulness.\n\nStable Diffusion - Stability AI released Stable Diffusion, a powerful open-source text-to-image model. Its open nature, in contrast to the closed models from OpenAI and Google, led to a massive explosion of community-driven innovation, tools, and applications in the AI art space.\n\n2023 - The LLM Arms Race Heats Up with Multimodality\n\nGPT-4 - OpenAI released GPT-4, its next-generation large language model. It was significantly more powerful than previous versions, exhibiting stronger reasoning, creativity, and problem-solving skills. Crucially, GPT-4 was designed as a large multimodal model, capable of accepting both text and image inputs, further expanding its capabilities.\n\nClaude - Anthropic, an AI safety and research company, released its family of models, including Claude and Claude Instant. Positioned as a direct competitor to GPT, Claude was developed with a strong emphasis on safety, using a technique called \"Constitutional AI\" to align the model with a set of explicit principles, offering a different approach to the alignment problem.\n\nGemini - Google DeepMind announced its Gemini family of models (Ultra, Pro, and Nano). Designed from the ground up to be natively multimodal, Gemini was positioned as Google's direct answer to GPT-4, capable of seamlessly understanding and operating across text, code, images, and video.\n\n2024 - Open Source Models Compete and AI Moves to Video\n\nClaude 3 - Anthropic released its Claude 3 model family (Haiku, Sonnet, and Opus). The most capable model, Opus, claimed to outperform GPT-4 on several industry benchmarks for the first time, including graduate-level reasoning and math, signaling a highly competitive landscape at the top end of AI capabilities.\n\nLlama 3 - Meta released Llama 3, its next-generation open-source model. It showed performance that was competitive with or superior to many powerful proprietary models, continuing the trend of high-quality open models democratizing access to cutting-edge AI and fostering a vibrant open-source ecosystem.\n\nSora - OpenAI previewed Sora, a text-to-video model capable of generating high-fidelity, minute-long video clips from simple text prompts. The model demonstrated a sophisticated understanding of the physical world, motion, and character consistency, representing a major leap forward for generative AI into the complex domain of video synthesis.",
      "reasoning": "",
      "usage": {
        "promptTokens": 194,
        "completionTokens": 4291,
        "completionTokensWithoutThoughts": 1828,
        "totalTokens": 4485,
        "thoughtsTokens": 2463
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 53223,
      "id": "f7f094b1-2ff2-4df9-9677-38feea56b0bc",
      "createdAt": "2025-07-21T12:49:55.639Z",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    },
    {
      "prompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nBe concise.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "context": [
        "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
      ],
      "model": "gemini-2.5-pro",
      "provider": "google",
      "rating": 8.75,
      "experimentId": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "promptStructure": "F",
      "images": [],
      "response": "2015 - Deep Learning Foundations\n\nResNet - Researchers at Microsoft, led by Kaiming He, introduced the Residual Network (ResNet). This architecture made it possible to train much deeper neural networks than ever before by using \"skip connections,\" which was a fundamental enabler for the massive models that would follow.\n\nDQN (Nature Paper) - Google DeepMind published its landmark paper on the Deep Q-Network (DQN) in Nature. This work demonstrated how a single algorithm could learn to master a wide range of Atari 2600 games directly from pixel inputs, effectively combining deep learning with reinforcement learning.\n\n2016 - Reinforcement Learning Masters a Classic Game\n\nAlphaGo - Google DeepMind created AlphaGo, the first computer program to defeat a professional human Go player. It combined Monte Carlo tree search (MCTS) with deep neural networks trained by supervised and reinforcement learning, a monumental achievement for AI and RL.\n\n2017 - The Transformer Architecture and RL Refinements\n\nAttention Is All You Need - Google researchers published this seminal paper, introducing the Transformer architecture. It replaced recurrent layers (RNNs) with self-attention mechanisms, allowing for parallel processing of input data. This architecture is the foundation of virtually all modern LLMs, including GPT and Claude.\n\nPPO - OpenAI released Proximal Policy Optimization (PPO), an RL algorithm that became a default choice for many applications due to its reliable performance and ease of implementation. It was later used in critical training steps like RLHF.\n\nAlphaGo Zero - Google DeepMind revealed AlphaGo Zero, which surpassed all previous versions of AlphaGo. Critically, it learned to play Go entirely through self-play, starting with no human data, demonstrating the power of pure reinforcement learning.\n\n2018 - Pre-trained Transformers Change NLP\n\nBERT - Google AI Language introduced Bidirectional Encoder Representations from Transformers (BERT). By pre-training a deep bidirectional model on a massive text corpus, BERT achieved state-of-the-art results on a wide range of language understanding tasks and popularized the concept of transfer learning in NLP.\n\nGPT-1 - OpenAI released its first Generative Pre-trained Transformer (GPT). This model established the effectiveness of the decoder-only Transformer architecture for generating coherent text and set the stage for its much larger successors.\n\n2019 - Generative Models Show Unprecedented Power\n\nGPT-2 - OpenAI developed GPT-2, a 1.5 billion parameter model that demonstrated a shocking ability to generate long, coherent, and stylistically versatile text. Initially deeming it too risky to fully release, its capabilities brought the potential of large language models into the public spotlight.\n\n2020 - The Era of Large Language Models Begins\n\nGPT-3 - OpenAI launched GPT-3, a massive leap to 175 billion parameters. It showed \"in-context learning,\" where it could perform tasks it was not explicitly trained on just by being given a few examples in the prompt. Its API access kicked off a wave of innovation in AI-powered applications.\n\n2022 - The Cambrian Explosion of Generative AI\n\nInstructGPT (RLHF) - OpenAI published its paper on InstructGPT, detailing the Reinforcement Learning from Human Feedback (RLHF) technique. This method fine-tuned models to better follow user instructions and align with human preferences, making them significantly more helpful and safer. This was the core technology behind ChatGPT.\n\nChatGPT - OpenAI released ChatGPT in November, a conversational AI built using the InstructGPT methodology. Its ease of use and remarkable ability to answer questions, write code, and draft text made it a global phenomenon, bringing the power of LLMs to hundreds of millions of users.\n\n2023 - The Great LLM Race\n\nGPT-4 - OpenAI released GPT-4, its next-generation model. It was multimodal, capable of processing both text and image inputs, and set a new standard for performance, reasoning, and accuracy across a wide variety of professional and academic benchmarks.\n\nClaude - Anthropic, a company founded by former OpenAI researchers, launched Claude. It was positioned as a major competitor focused on safety, trained using a novel technique called \"Constitutional AI\" to make it more helpful and harmless.\n\nLlama 2 - Meta released Llama 2, a family of pre-trained and fine-tuned LLMs made available for free for most research and commercial use. This move significantly boosted the open-source AI community, providing a powerful alternative to closed, proprietary models.\n\n2024 - Context Windows and Open Source Advance\n\nClaude 3 - Anthropic released its Claude 3 family of models (Haiku, Sonnet, and Opus). The most capable model, Opus, became the first widely available model to claim superior performance over GPT-4 on several key industry benchmarks.\n\nGemini 1.5 Pro - Google demonstrated a major technical breakthrough with Gemini 1.5 Pro, featuring a massive 1 million token context window. This allows the model to process and reason over huge amounts of information, such as entire codebases or hours of video, in a single prompt.\n\nLlama 3 - Meta released its next generation of open-source models, Llama 3. The models showed greatly improved performance, further closing the gap between the best open-source and proprietary models and continuing to empower developers worldwide.",
      "reasoning": "",
      "usage": {
        "promptTokens": 198,
        "completionTokens": 3594,
        "completionTokensWithoutThoughts": 1104,
        "totalTokens": 3792,
        "thoughtsTokens": 2490
      },
      "fullPrompt": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nBe concise.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n\n\nInclude noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n",
      "durationMs": 43533,
      "id": "a40bc0fc-c42d-4a21-aec3-a62269aa3acb",
      "createdAt": "2025-07-21T12:51:37.680Z",
      "notes": "Covers major points",
      "contextId": [
        "d6cd5121-f2de-43d9-810f-a018885ef35d"
      ]
    }
  ],
  "experiments": [
    {
      "id": "29ecce6c-fe14-4fd5-a085-9bf0e6759d70",
      "name": "Writing an AI Timeline (Kimi)",
      "category": "writing",
      "createdAt": "2025-07-18T06:57:32.740Z",
      "evalFunctionId": "d5f2d0c1-f1a0-47aa-853f-fc099f11ca34"
    }
  ],
  "contexts": [
    {
      "id": "d6cd5121-f2de-43d9-810f-a018885ef35d",
      "name": "Sample AI timeline context",
      "type": "text",
      "content": "Include noteworthy publications, algorithms, events and products. For example, DQN, PPO, BERT and RLHF.\n"
    }
  ],
  "prompts": [
    {
      "name": "AI timeline",
      "content": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "id": "18dd3aea-a9c5-4a8f-8ffa-f73f9bafbfc5",
      "createdAt": "2025-04-29T11:52:18.296Z",
      "hash": "22a73396",
      "category": "writing"
    },
    {
      "name": "AI timeline v2 (concise)",
      "content": "Compile a chronological timeline of important developments in AI and RL in recent years from 2015 to 2024, leading to the invention of LLMs and powerful models like GPT and Claude.\n\nWrite a short paragraph for each important milestone. Include the company or person responsible for it.\n\nRespond in plain text with minimal formatting.\n\nBe concise.\n\nUse the following format:\n\nYear - {Head line}\n\n{Milestone 1} - description\n\n{Milestone 2} - description\n\n{Milestone 3} - description\n\n...\n\nHere is an example of the milestone for AlphaGo:\n\nAlphaGo - Google DeepMind created AplphaGo, the first computer program to defeat a professional human Go player, combining Monte Carlo tree search (MCST) with deep neural networks trained by supervised and reinforcement learning.\n",
      "id": "d421b1c0-5b58-47ec-a28a-ea175910b249",
      "createdAt": "2025-05-08T09:50:35.047Z",
      "hash": "6f825f09",
      "category": "writing"
    }
  ]
}